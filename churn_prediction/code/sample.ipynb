{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "#preprocessing \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "#import libraries for bias and explainability\n",
    "import sagemaker\n",
    "from sagemaker import Session\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import clarify\n",
    "\n",
    "#library for privacy engine\n",
    "from opacus import PrivacyEngine\n",
    "\n",
    "#library for carbon emission\n",
    "from codecarbon import EmissionsTracker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data path = s3://responsibleai/chrun_prediction/data\n",
      "Bias report path = s3://responsibleai/chrun_prediction/bias_explain/clarify-bias\n",
      "Explainability report path = s3://responsibleai/chrun_prediction/bias_explain/clarify-explainability\n"
     ]
    }
   ],
   "source": [
    "# Get the session, region and role\n",
    "session = Session() \n",
    "region = session.boto_region_name \n",
    "role = get_execution_role() \n",
    "s3_client = boto3.client(\"s3\") \n",
    "\n",
    "# Set up prefix for data and outputs\n",
    "bucket = \"responsibleai\"\n",
    "data_prefix = \"chrun_prediction/data\"\n",
    "bias_prefix = \"chrun_prediction/bias_explain\" \n",
    "\n",
    "input_data_path = \"s3://{}/{}\".format(bucket, data_prefix)\n",
    "explainability_output_path = \"s3://{}/{}/clarify-explainability\".format(bucket, bias_prefix) \n",
    "bias_report_output_path = \"s3://{}/{}/clarify-bias\".format(bucket, bias_prefix)\n",
    "\n",
    "# initialize few params\n",
    "csv_file = \"../data/churn.csv\"\n",
    "batch_size = 50\n",
    "\n",
    "print(\"Input data path = \" + input_data_path)\n",
    "print(\"Bias report path = \" + bias_report_output_path)\n",
    "print(\"Explainability report path = \" + explainability_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CreditScore  Age  Tenure    Balance  NumOfProducts  HasCrCard  \\\n",
      "0          619   42       2       0.00              1          1   \n",
      "1          608   41       1   83807.86              1          0   \n",
      "2          502   42       8  159660.80              3          1   \n",
      "3          699   39       1       0.00              2          0   \n",
      "4          850   43       2  125510.82              1          1   \n",
      "\n",
      "   IsActiveMember  EstimatedSalary  Geography_France  Geography_Germany  \\\n",
      "0               1        101348.88                 1                  0   \n",
      "1               1        112542.58                 0                  0   \n",
      "2               0        113931.57                 1                  0   \n",
      "3               0         93826.63                 1                  0   \n",
      "4               1         79084.10                 0                  0   \n",
      "\n",
      "   Geography_Spain  Gender_Female  Gender_Male  \n",
      "0                0              1            0  \n",
      "1                1              1            0  \n",
      "2                0              1            0  \n",
      "3                0              1            0  \n",
      "4                1              1            0  \n",
      "   CreditScore       Age    Tenure   Balance  NumOfProducts  HasCrCard  \\\n",
      "0    -0.326221  0.293517 -1.041760 -1.225848      -0.911583   0.646092   \n",
      "1    -0.440036  0.198164 -1.387538  0.117350      -0.911583  -1.547768   \n",
      "2    -1.536794  0.293517  1.032908  1.333053       2.527057   0.646092   \n",
      "3     0.501521  0.007457 -1.387538 -1.225848       0.807737  -1.547768   \n",
      "4     2.063884  0.388871 -1.041760  0.785728      -0.911583   0.646092   \n",
      "\n",
      "   IsActiveMember  EstimatedSalary  Geography_France  Geography_Germany  \\\n",
      "0        0.970243         0.021886          0.997204          -0.578736   \n",
      "1        0.970243         0.216534         -1.002804          -0.578736   \n",
      "2       -1.030670         0.240687          0.997204          -0.578736   \n",
      "3       -1.030670        -0.108918          0.997204          -0.578736   \n",
      "4        0.970243        -0.365276         -1.002804          -0.578736   \n",
      "\n",
      "   Geography_Spain  Gender_Female  Gender_Male  \n",
      "0        -0.573809       1.095988    -1.095988  \n",
      "1         1.742740       1.095988    -1.095988  \n",
      "2        -0.573809       1.095988    -1.095988  \n",
      "3        -0.573809       1.095988    -1.095988  \n",
      "4         1.742740       1.095988    -1.095988  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "df = df.drop([\"Surname\", \"CustomerId\", \"RowNumber\"], axis=1)\n",
    "\n",
    "# Grouping variable names\n",
    "categorical = [\"Geography\", \"Gender\"]\n",
    "target = \"Exited\"\n",
    "\n",
    "# One-hot encoding of categorical variables\n",
    "training_data = pd.get_dummies(df, prefix=categorical)\n",
    "\n",
    "churn_features = training_data.drop(['Exited'], axis = 1)\n",
    "churn_label = training_date['Exited']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_array  = scaler.fit_transform(churn_features)\n",
    "X = pd.DataFrame(X_array, columns = churn_features.columns)\n",
    "\n",
    "#churn_train, churn_test = train_test_split (training_data, test_size=0.2)\n",
    "\n",
    "print(churn_features.head())\n",
    "print(X.head())\n",
    "#churn_train = pd.concat([churn_train[\"Exited\"], churn_train.drop([\"Exited\"], axis=1)], axis=1)\n",
    "\n",
    "#X = churn_frame.drop(target, axis=1)\n",
    "#y = churn_frame[\"Exited\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChurnDataSet(Dataset):\n",
    " \n",
    "    def __init__(self, df_features, df_target):\n",
    "        self.X = df_features\n",
    "        self.y = df_target\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_array  = scaler.fit_transform(self.X)\n",
    "        self.X = pd.DataFrame(X_array)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.churn_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert idx from tensor to list due to pandas bug (that arises when using pytorch's random_split)\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        return [self.X.iloc[idx].values, self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareDataSet(csv_file):   \n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        df = df.drop([\"Surname\", \"CustomerId\", \"RowNumber\"], axis=1)\n",
    "\n",
    "        # Grouping variable names\n",
    "        categorical = [\"Geography\", \"Gender\"]\n",
    "        target = \"Exited\"\n",
    "\n",
    "        # One-hot encoding of categorical variables\n",
    "        churn_frame = pd.get_dummies(df, prefix=categorical)\n",
    "\n",
    "        # Save target and predictors\n",
    "        X = churn_frame.drop(target, axis=1)\n",
    "        y = churn_frame[\"Exited\"]\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CHURN_model():\n",
    "    model = nn.Sequential(nn.Linear(13, 64), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(64, 64), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(64, 1)) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(csv_file, batch_size):\n",
    "    \n",
    "    # Split into training and test\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    trainset, testset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return trainloader, testloader, trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, net, optimizer, n_epochs=100):\n",
    "     \n",
    "    device = \"cpu\"\n",
    "\n",
    "    # Define the model\n",
    "    #net = get_CHURN_model()\n",
    "    net = net.to(device)\n",
    "    \n",
    "    #criterion = nn.CrossEntropyLoss() \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Train the net\n",
    "    loss_per_iter = []\n",
    "    loss_per_batch = []\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward + backward + optimize\n",
    "            outputs = net(inputs.float())\n",
    "            loss = criterion(outputs, labels.float().unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Save loss to plot\n",
    "            running_loss += loss.item()\n",
    "            loss_per_iter.append(loss.item())\n",
    "\n",
    "        \n",
    "        print(\"Epoch {} - Training loss: {}\".format(epoch, running_loss/len(trainloader))) \n",
    "        \n",
    "        running_loss = 0.0\n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boto3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-15cbd3adfeb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mregion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboto_region_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_execution_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0ms3_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Set up prefix for data and outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'boto3' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.326221</td>\n",
       "      <td>0.293517</td>\n",
       "      <td>-1.041760</td>\n",
       "      <td>-1.225848</td>\n",
       "      <td>-0.911583</td>\n",
       "      <td>0.646092</td>\n",
       "      <td>0.970243</td>\n",
       "      <td>0.021886</td>\n",
       "      <td>0.997204</td>\n",
       "      <td>-0.578736</td>\n",
       "      <td>-0.573809</td>\n",
       "      <td>1.095988</td>\n",
       "      <td>-1.095988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.440036</td>\n",
       "      <td>0.198164</td>\n",
       "      <td>-1.387538</td>\n",
       "      <td>0.117350</td>\n",
       "      <td>-0.911583</td>\n",
       "      <td>-1.547768</td>\n",
       "      <td>0.970243</td>\n",
       "      <td>0.216534</td>\n",
       "      <td>-1.002804</td>\n",
       "      <td>-0.578736</td>\n",
       "      <td>1.742740</td>\n",
       "      <td>1.095988</td>\n",
       "      <td>-1.095988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.536794</td>\n",
       "      <td>0.293517</td>\n",
       "      <td>1.032908</td>\n",
       "      <td>1.333053</td>\n",
       "      <td>2.527057</td>\n",
       "      <td>0.646092</td>\n",
       "      <td>-1.030670</td>\n",
       "      <td>0.240687</td>\n",
       "      <td>0.997204</td>\n",
       "      <td>-0.578736</td>\n",
       "      <td>-0.573809</td>\n",
       "      <td>1.095988</td>\n",
       "      <td>-1.095988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.501521</td>\n",
       "      <td>0.007457</td>\n",
       "      <td>-1.387538</td>\n",
       "      <td>-1.225848</td>\n",
       "      <td>0.807737</td>\n",
       "      <td>-1.547768</td>\n",
       "      <td>-1.030670</td>\n",
       "      <td>-0.108918</td>\n",
       "      <td>0.997204</td>\n",
       "      <td>-0.578736</td>\n",
       "      <td>-0.573809</td>\n",
       "      <td>1.095988</td>\n",
       "      <td>-1.095988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.063884</td>\n",
       "      <td>0.388871</td>\n",
       "      <td>-1.041760</td>\n",
       "      <td>0.785728</td>\n",
       "      <td>-0.911583</td>\n",
       "      <td>0.646092</td>\n",
       "      <td>0.970243</td>\n",
       "      <td>-0.365276</td>\n",
       "      <td>-1.002804</td>\n",
       "      <td>-0.578736</td>\n",
       "      <td>1.742740</td>\n",
       "      <td>1.095988</td>\n",
       "      <td>-1.095988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.326221  0.293517 -1.041760 -1.225848 -0.911583  0.646092  0.970243   \n",
       "1 -0.440036  0.198164 -1.387538  0.117350 -0.911583 -1.547768  0.970243   \n",
       "2 -1.536794  0.293517  1.032908  1.333053  2.527057  0.646092 -1.030670   \n",
       "3  0.501521  0.007457 -1.387538 -1.225848  0.807737 -1.547768 -1.030670   \n",
       "4  2.063884  0.388871 -1.041760  0.785728 -0.911583  0.646092  0.970243   \n",
       "\n",
       "         7         8         9         10        11        12  \n",
       "0  0.021886  0.997204 -0.578736 -0.573809  1.095988 -1.095988  \n",
       "1  0.216534 -1.002804 -0.578736  1.742740  1.095988 -1.095988  \n",
       "2  0.240687  0.997204 -0.578736 -0.573809  1.095988 -1.095988  \n",
       "3 -0.108918  0.997204 -0.578736 -0.573809  1.095988 -1.095988  \n",
       "4 -0.365276 -1.002804 -0.578736  1.742740  1.095988 -1.095988  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# prepare dataset\n",
    "X, y = PrepareDataSet(csv_file)\n",
    "\n",
    "\n",
    "#train and test split\n",
    "#churn_train, churn_test = train_test_split (X, test_size=0.2)\n",
    "#churn_train.to_csv(\"../data/train_churn.csv\", index=False, header=False)\n",
    "\n",
    "\n",
    "# convert dataset for pytorch\n",
    "#trainloader, testloader, train_ds, test_ds = get_dataloader(csv_file, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-02-28 15:21:54.261 pytorch-1-8-gpu-py3-ml-g4dn-xlarge-60bd0d07a83be181dcf7335baae2:1222 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-02-28 15:21:54.294 pytorch-1-8-gpu-py3-ml-g4dn-xlarge-60bd0d07a83be181dcf7335baae2:1222 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Epoch 0 - Training loss: 0.4253511395305395\n",
      "Epoch 1 - Training loss: 0.3550588250160217\n",
      "Epoch 2 - Training loss: 0.3438827622681856\n",
      "Epoch 3 - Training loss: 0.3400544809643179\n",
      "Epoch 4 - Training loss: 0.3367278458550572\n",
      "Epoch 5 - Training loss: 0.3320686207152903\n",
      "Epoch 6 - Training loss: 0.33003695383667947\n",
      "Epoch 7 - Training loss: 0.3285841692239046\n",
      "Epoch 8 - Training loss: 0.32846663082018496\n",
      "Epoch 9 - Training loss: 0.32431351514533163\n",
      "Epoch 10 - Training loss: 0.3216641909442842\n",
      "Epoch 11 - Training loss: 0.3224272786639631\n",
      "Epoch 12 - Training loss: 0.3198192465119064\n",
      "Epoch 13 - Training loss: 0.31969332657754423\n",
      "Epoch 14 - Training loss: 0.31759447380900385\n",
      "Epoch 15 - Training loss: 0.3154820871539414\n",
      "Epoch 16 - Training loss: 0.31491048661991955\n",
      "Epoch 17 - Training loss: 0.3124771511182189\n",
      "Epoch 18 - Training loss: 0.313001093454659\n",
      "Epoch 19 - Training loss: 0.3114446332678199\n",
      "Epoch 20 - Training loss: 0.3081054063513875\n",
      "Epoch 21 - Training loss: 0.3060579590499401\n",
      "Epoch 22 - Training loss: 0.30475006327033044\n",
      "Epoch 23 - Training loss: 0.3044068248011172\n",
      "Epoch 24 - Training loss: 0.3030420479364693\n",
      "Epoch 25 - Training loss: 0.30002744011580945\n",
      "Epoch 26 - Training loss: 0.29841640321537855\n",
      "Epoch 27 - Training loss: 0.2979358990676701\n",
      "Epoch 28 - Training loss: 0.29824280650354923\n",
      "Epoch 29 - Training loss: 0.2925468265078962\n",
      "Epoch 30 - Training loss: 0.29243835117667916\n",
      "Epoch 31 - Training loss: 0.2915194950066507\n",
      "Epoch 32 - Training loss: 0.2878848729655147\n",
      "Epoch 33 - Training loss: 0.2857625263277441\n",
      "Epoch 34 - Training loss: 0.284910985827446\n",
      "Epoch 35 - Training loss: 0.2850962353870273\n",
      "Epoch 36 - Training loss: 0.2834230745676905\n",
      "Epoch 37 - Training loss: 0.2800748914014548\n",
      "Epoch 38 - Training loss: 0.27706146012060345\n",
      "Epoch 39 - Training loss: 0.27617972348816694\n",
      "Epoch 40 - Training loss: 0.27707540467381475\n",
      "Epoch 41 - Training loss: 0.2750866358634084\n",
      "Epoch 42 - Training loss: 0.2683896921109408\n",
      "Epoch 43 - Training loss: 0.2714035692624748\n",
      "Epoch 44 - Training loss: 0.2707118073478341\n",
      "Epoch 45 - Training loss: 0.2686799531802535\n",
      "Epoch 46 - Training loss: 0.2665183037985116\n",
      "Epoch 47 - Training loss: 0.26457861755043266\n",
      "Epoch 48 - Training loss: 0.2610290027689189\n",
      "Epoch 49 - Training loss: 0.26011678953655065\n"
     ]
    }
   ],
   "source": [
    "\n",
    "net = get_CHURN_model()\n",
    "optimizer = optim.Adam(net.parameters(), weight_decay=0.0001, lr=0.003)\n",
    "model = train(trainloader, net, optimizer, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_per_sample_grad_norm = 1.5\n",
    "sample_rate = batch_size/len(train_ds)\n",
    "noise_multiplier = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/opacus/privacy_engine.py:760: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
      "  \"A ``sample_rate`` has been provided.\"\n",
      "/opt/conda/lib/python3.6/site-packages/opacus/privacy_engine.py:237: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:796: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.5739273639395833\n",
      "Epoch 1 - Training loss: 0.5450084384530782\n",
      "Epoch 2 - Training loss: 0.5399808972142637\n",
      "Epoch 3 - Training loss: 0.5380573300644755\n",
      "Epoch 4 - Training loss: 0.5396806959062814\n",
      "Epoch 5 - Training loss: 0.5379347333684563\n",
      "Epoch 6 - Training loss: 0.5379639642313123\n",
      "Epoch 7 - Training loss: 0.5341536544263363\n",
      "Epoch 8 - Training loss: 0.5255999676883221\n",
      "Epoch 9 - Training loss: 0.527379980077967\n",
      "Epoch 10 - Training loss: 0.5191250344272703\n",
      "Epoch 11 - Training loss: 0.5165148722939193\n",
      "Epoch 12 - Training loss: 0.5069711779244244\n",
      "Epoch 13 - Training loss: 0.5001096079126001\n",
      "Epoch 14 - Training loss: 0.5008684289641678\n",
      "Epoch 15 - Training loss: 0.4996961490251124\n",
      "Epoch 16 - Training loss: 0.49414023593999445\n",
      "Epoch 17 - Training loss: 0.4997370705939829\n",
      "Epoch 18 - Training loss: 0.5062210355419665\n",
      "Epoch 19 - Training loss: 0.5157987097278237\n",
      "Epoch 20 - Training loss: 0.5177786984480918\n",
      "Epoch 21 - Training loss: 0.5228384896647185\n",
      "Epoch 22 - Training loss: 0.5220020848326385\n",
      "Epoch 23 - Training loss: 0.5266515244729817\n",
      "Epoch 24 - Training loss: 0.5171955246478319\n",
      "Epoch 25 - Training loss: 0.5184680629987269\n",
      "Epoch 26 - Training loss: 0.5142837151885032\n",
      "Epoch 27 - Training loss: 0.5262248961254954\n",
      "Epoch 28 - Training loss: 0.5282597417244688\n",
      "Epoch 29 - Training loss: 0.5250876088626683\n",
      "Epoch 30 - Training loss: 0.5199503113050014\n",
      "Epoch 31 - Training loss: 0.5246013172902166\n",
      "Epoch 32 - Training loss: 0.5226997941732406\n",
      "Epoch 33 - Training loss: 0.5192192185670137\n",
      "Epoch 34 - Training loss: 0.5192204292165116\n",
      "Epoch 35 - Training loss: 0.5268803596496582\n",
      "Epoch 36 - Training loss: 0.516181671526283\n",
      "Epoch 37 - Training loss: 0.5162922478630207\n",
      "Epoch 38 - Training loss: 0.5269118731841445\n",
      "Epoch 39 - Training loss: 0.5253719016909599\n",
      "Epoch 40 - Training loss: 0.5307155544869602\n",
      "Epoch 41 - Training loss: 0.531667483272031\n",
      "Epoch 42 - Training loss: 0.5294012497644871\n",
      "Epoch 43 - Training loss: 0.5296641108114273\n",
      "Epoch 44 - Training loss: 0.5223643764387816\n",
      "Epoch 45 - Training loss: 0.528047192748636\n",
      "Epoch 46 - Training loss: 0.5401560562255326\n",
      "Epoch 47 - Training loss: 0.537497458839789\n",
      "Epoch 48 - Training loss: 0.5318105593323708\n",
      "Epoch 49 - Training loss: 0.5288757191970944\n"
     ]
    }
   ],
   "source": [
    "tracker = EmissionsTracker(project_name = \"churn_prediction\",\n",
    "                           output_dir = \"../output/\",\n",
    "                           measure_power_secs = 15,\n",
    "                           save_to_file = True)\n",
    "\n",
    "tracker.start()\n",
    "\n",
    "net = get_CHURN_model()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), weight_decay=0.0001, lr=0.003)\n",
    "\n",
    "privacy_engine = PrivacyEngine(\n",
    "    net,\n",
    "    max_grad_norm=max_per_sample_grad_norm,\n",
    "    noise_multiplier = noise_multiplier,\n",
    "    sample_rate = sample_rate,\n",
    ")\n",
    "\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "model = train(trainloader, net, optimizer, batch_size)\n",
    "\n",
    "emissions: float = tracker.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ε = 6.39, δ = 1e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"**** Differential Privacy *******\")\n",
    "epsilon, best_alpha = privacy_engine.get_privacy_spent()\n",
    "print (f\" ε = {epsilon:.2f}, δ = {privacy_engine.target_delta}\")\n",
    "\n",
    "print(\"**** Emissions Information*******\")\n",
    "print(emissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uri = S3Uploader.upload(\"../data/train_churn.csv\", \"s3://{}/{}\".format(bucket, data_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clarify_processor = clarify.SageMakerClarifyProcessor( \n",
    "                        role=role, \n",
    "                        instance_count=1, \n",
    "                        instance_type=\"ml.m5.xlarge\", \n",
    "                        sagemaker_session=session) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_data_config = clarify.DataConfig( \n",
    "    s3_data_input_path=train_uri, \n",
    "    s3_output_path=bias_report_output_path, \n",
    "    label=\"Exited\", \n",
    "    headers=churn_train.columns.to_list(), \n",
    "    dataset_type=\"text/csv\") \n",
    "\n",
    "model_config = clarify.ModelConfig( \n",
    "    model_name=model_name, \n",
    "    instance_type=\"ml.m5.xlarge\", \n",
    "    instance_count=1,\n",
    "    accept_type=\"text/csv\", \n",
    "    content_type=\"text/csv\",) \n",
    "\n",
    "predictions_config = clarify.ModelPredictedLabelConfig(probability_threshold=0.8)\n",
    "\n",
    "bias_config = clarify.BiasConfig( \n",
    "    label_values_or_threshold=[1], \n",
    "    facet_name=\"Gender\", \n",
    "    facet_values_or_threshold=[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clarify_processor.run_bias( \n",
    "    data_config=bias_data_config, \n",
    "    bias_config=bias_config, \n",
    "    model_config=model_config, \n",
    "    model_predicted_label_config=predictions_config, \n",
    "    pre_training_methods=\"all\", \n",
    "    post_training_methods=\"all\") "
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.8-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
