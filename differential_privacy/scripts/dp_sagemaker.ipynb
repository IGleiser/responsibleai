{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_estimator = PyTorch(entry_point = '../code/train.py',\n",
    "                            source_dir = '../code',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            role = role,\n",
    "                            instance_count=1,\n",
    "                            framework_version='1.8.0',\n",
    "                            py_version='py3',\n",
    "                            hyperparameters = {'epochs': 100, \n",
    "                                               'batch-size': 64, \n",
    "                                               'learning-rate': 0.003})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-28 20:23:36 Starting - Starting the training job...\n",
      "2022-02-28 20:23:39 Starting - Launching requested ML instancesProfilerReport-1646079816: InProgress\n",
      ".........\n",
      "2022-02-28 20:25:35 Starting - Preparing the instances for training.........\n",
      "2022-02-28 20:26:55 Downloading - Downloading input data\n",
      "2022-02-28 20:26:55 Training - Downloading the training image........................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-02-28 20:31:03,374 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-02-28 20:31:03,399 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-02-28 20:31:06,459 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-02-28 20:31:06,797 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting opacus\n",
      "  Downloading opacus-0.15.0-py3-none-any.whl (125 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.6/site-packages (from opacus->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.2 in /opt/conda/lib/python3.6/site-packages (from opacus->-r requirements.txt (line 1)) (1.5.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from opacus->-r requirements.txt (line 1)) (1.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch>=1.3->opacus->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3->opacus->-r requirements.txt (line 1)) (3.7.4.3)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: opacus\u001b[0m\n",
      "\u001b[34mSuccessfully installed opacus-0.15.0\u001b[0m\n",
      "\u001b[34m2022-02-28 20:31:09,192 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 64,\n",
      "        \"learning-rate\": 0.003,\n",
      "        \"epochs\": 100\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-02-28-20-23-36-362\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-247385444016/pytorch-training-2022-02-28-20-23-36-362/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"../code/train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"../code/train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":64,\"epochs\":100,\"learning-rate\":0.003}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=../code/train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=../code/train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-247385444016/pytorch-training-2022-02-28-20-23-36-362/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":64,\"epochs\":100,\"learning-rate\":0.003},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-02-28-20-23-36-362\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-247385444016/pytorch-training-2022-02-28-20-23-36-362/source/sourcedir.tar.gz\",\"module_name\":\"../code/train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"../code/train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"64\",\"--epochs\",\"100\",\"--learning-rate\",\"0.003\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.003\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=100\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 ../code/train.py --batch-size 64 --epochs 100 --learning-rate 0.003\u001b[0m\n",
      "\u001b[34m[2022-02-28 20:31:12.973 algo-1:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-02-28 20:31:13.109 algo-1:32 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-02-28 20:31:13.109 algo-1:32 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-02-28 20:31:13.110 algo-1:32 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-02-28 20:31:13.110 algo-1:32 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-02-28 20:31:13.111 algo-1:32 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-02-28 20:31:13.594 algo-1:32 INFO hook.py:584] name:0.weight count_params:832\u001b[0m\n",
      "\u001b[34m[2022-02-28 20:31:13.594 algo-1:32 INFO hook.py:584] name:0.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2022-02-28 20:31:13.594 algo-1:32 INFO hook.py:584] name:2.weight count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-02-28 20:31:13.594 algo-1:32 INFO hook.py:584] name:2.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2022-02-28 20:31:13.594 algo-1:32 INFO hook.py:584] name:4.weight count_params:64\u001b[0m\n",
      "\u001b[34m[2022-02-28 20:31:13.594 algo-1:32 INFO hook.py:584] name:4.bias count_params:1\u001b[0m\n",
      "\u001b[34m[2022-02-28 20:31:13.594 algo-1:32 INFO hook.py:586] Total Trainable Params: 5121\u001b[0m\n",
      "\u001b[34m[2022-02-28 20:31:13.594 algo-1:32 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-02-28 20:31:13.597 algo-1:32 INFO hook.py:476] Hook is writing from the hook with pid: 32\u001b[0m\n",
      "\u001b[34mEpoch 0 - Training loss: 0.4292893116362393\u001b[0m\n",
      "\u001b[34mEpoch 1 - Training loss: 0.357119667250663\u001b[0m\n",
      "\u001b[34mEpoch 2 - Training loss: 0.3443308443762362\u001b[0m\n",
      "\u001b[34mEpoch 3 - Training loss: 0.3374265861697495\u001b[0m\n",
      "\u001b[34mEpoch 4 - Training loss: 0.3347501169890165\u001b[0m\n",
      "\u001b[34mEpoch 5 - Training loss: 0.3306068217381835\u001b[0m\n",
      "\u001b[34mEpoch 6 - Training loss: 0.32827306305989623\u001b[0m\n",
      "\u001b[34mEpoch 7 - Training loss: 0.3270724448375404\u001b[0m\n",
      "\u001b[34mEpoch 8 - Training loss: 0.3239821045659482\u001b[0m\n",
      "\u001b[34mEpoch 9 - Training loss: 0.3238842707127333\u001b[0m\n",
      "\u001b[34mEpoch 10 - Training loss: 0.3200530191883445\u001b[0m\n",
      "\u001b[34mEpoch 11 - Training loss: 0.319714479893446\u001b[0m\n",
      "\u001b[34mEpoch 12 - Training loss: 0.31836180537939074\u001b[0m\n",
      "\u001b[34mEpoch 13 - Training loss: 0.3185383778065443\u001b[0m\n",
      "\u001b[34mEpoch 14 - Training loss: 0.3136386478319764\u001b[0m\n",
      "\u001b[34mEpoch 15 - Training loss: 0.3142100186087191\u001b[0m\n",
      "\u001b[34mEpoch 16 - Training loss: 0.3121291543357074\u001b[0m\n",
      "\u001b[34mEpoch 17 - Training loss: 0.3109075157903135\u001b[0m\n",
      "\u001b[34mEpoch 18 - Training loss: 0.3097816957626492\u001b[0m\n",
      "\u001b[34mEpoch 19 - Training loss: 0.30508149405941365\u001b[0m\n",
      "\u001b[34mEpoch 20 - Training loss: 0.305768814496696\u001b[0m\n",
      "\u001b[34mEpoch 21 - Training loss: 0.300676254555583\u001b[0m\n",
      "\u001b[34mEpoch 22 - Training loss: 0.2992095514200628\u001b[0m\n",
      "\u001b[34mEpoch 23 - Training loss: 0.30027300417423247\u001b[0m\n",
      "\u001b[34mEpoch 24 - Training loss: 0.29951558937318623\u001b[0m\n",
      "\u001b[34mEpoch 25 - Training loss: 0.2979729440063238\u001b[0m\n",
      "\u001b[34mEpoch 26 - Training loss: 0.29381288518197834\u001b[0m\n",
      "\u001b[34mEpoch 27 - Training loss: 0.29418412055820226\u001b[0m\n",
      "\u001b[34mEpoch 28 - Training loss: 0.2892044265754521\u001b[0m\n",
      "\u001b[34mEpoch 29 - Training loss: 0.2885482933372259\u001b[0m\n",
      "\n",
      "2022-02-28 20:31:57 Training - Training image download completed. Training in progress.\u001b[34mEpoch 30 - Training loss: 0.28634312325157224\u001b[0m\n",
      "\u001b[34mEpoch 31 - Training loss: 0.28635027501732113\u001b[0m\n",
      "\u001b[34mEpoch 32 - Training loss: 0.28416971522383394\u001b[0m\n",
      "\u001b[34mEpoch 33 - Training loss: 0.28016005135141314\u001b[0m\n",
      "\u001b[34mEpoch 34 - Training loss: 0.2777923100162297\u001b[0m\n",
      "\u001b[34mEpoch 35 - Training loss: 0.2783629544544965\u001b[0m\n",
      "\u001b[34mEpoch 36 - Training loss: 0.27710750899277625\u001b[0m\n",
      "\u001b[34mEpoch 37 - Training loss: 0.2711290913168341\u001b[0m\n",
      "\u001b[34mEpoch 38 - Training loss: 0.27174133942462503\u001b[0m\n",
      "\u001b[34mEpoch 39 - Training loss: 0.2710308943875134\u001b[0m\n",
      "\u001b[34mEpoch 40 - Training loss: 0.2688744434155524\u001b[0m\n",
      "\u001b[34mEpoch 41 - Training loss: 0.2649003766942769\u001b[0m\n",
      "\u001b[34mEpoch 42 - Training loss: 0.2657053124625236\u001b[0m\n",
      "\u001b[34mEpoch 43 - Training loss: 0.2606580769177526\u001b[0m\n",
      "\u001b[34mEpoch 44 - Training loss: 0.260411124303937\u001b[0m\n",
      "\u001b[34mEpoch 45 - Training loss: 0.25803888253867624\u001b[0m\n",
      "\u001b[34mEpoch 46 - Training loss: 0.2562749699689448\u001b[0m\n",
      "\u001b[34mEpoch 47 - Training loss: 0.2553557673003525\u001b[0m\n",
      "\u001b[34mEpoch 48 - Training loss: 0.25186902447603643\u001b[0m\n",
      "\u001b[34mEpoch 49 - Training loss: 0.25105440746992824\u001b[0m\n",
      "\u001b[34mEpoch 0 - Training loss: 0.5400652438402176\u001b[0m\n",
      "\u001b[34mEpoch 1 - Training loss: 0.5208411401137709\u001b[0m\n",
      "\u001b[34mEpoch 2 - Training loss: 0.5234237719327212\u001b[0m\n",
      "\u001b[34mEpoch 3 - Training loss: 0.5251003899145872\u001b[0m\n",
      "\u001b[34mEpoch 4 - Training loss: 0.5247230485081673\u001b[0m\n",
      "\u001b[34mEpoch 5 - Training loss: 0.5214368591085077\u001b[0m\n",
      "\u001b[34mEpoch 6 - Training loss: 0.5049602339044214\u001b[0m\n",
      "\u001b[34mEpoch 7 - Training loss: 0.5166678311303258\u001b[0m\n",
      "\u001b[34mEpoch 8 - Training loss: 0.5167588074691594\u001b[0m\n",
      "\u001b[34mEpoch 9 - Training loss: 0.504300500266254\u001b[0m\n",
      "\u001b[34mEpoch 10 - Training loss: 0.5044300188776105\u001b[0m\n",
      "\u001b[34mEpoch 11 - Training loss: 0.5054594172164798\u001b[0m\n",
      "\u001b[34mEpoch 12 - Training loss: 0.501024019625038\u001b[0m\n",
      "\u001b[34mEpoch 13 - Training loss: 0.5056330423802138\u001b[0m\n",
      "\u001b[34mEpoch 14 - Training loss: 0.504573117941618\u001b[0m\n",
      "\u001b[34mEpoch 15 - Training loss: 0.505471202172339\u001b[0m\n",
      "\u001b[34mEpoch 16 - Training loss: 0.5050939091481268\u001b[0m\n",
      "\u001b[34mEpoch 17 - Training loss: 0.5164783427491784\u001b[0m\n",
      "\u001b[34mEpoch 18 - Training loss: 0.508115123026073\u001b[0m\n",
      "\u001b[34mEpoch 19 - Training loss: 0.5128582129720598\u001b[0m\n",
      "\u001b[34mEpoch 20 - Training loss: 0.5180769418366253\u001b[0m\n",
      "\u001b[34mEpoch 21 - Training loss: 0.5147828319109976\u001b[0m\n",
      "\u001b[34mEpoch 22 - Training loss: 0.5150693258037791\u001b[0m\n",
      "\u001b[34mEpoch 23 - Training loss: 0.515623223548755\u001b[0m\n",
      "\u001b[34mEpoch 24 - Training loss: 0.5244460214511492\u001b[0m\n",
      "\u001b[34mEpoch 25 - Training loss: 0.532377426698804\u001b[0m\n",
      "\u001b[34mEpoch 26 - Training loss: 0.5310727071017027\u001b[0m\n",
      "\u001b[34mEpoch 27 - Training loss: 0.5392737254500389\u001b[0m\n",
      "\u001b[34mEpoch 28 - Training loss: 0.5364202922442928\u001b[0m\n",
      "\u001b[34mEpoch 29 - Training loss: 0.5400130696594715\u001b[0m\n",
      "\u001b[34mEpoch 30 - Training loss: 0.543344462569803\u001b[0m\n",
      "\u001b[34mEpoch 31 - Training loss: 0.5498097490984947\u001b[0m\n",
      "\u001b[34mEpoch 32 - Training loss: 0.5405806853203103\u001b[0m\n",
      "\u001b[34mEpoch 33 - Training loss: 0.5427642275113612\u001b[0m\n",
      "\u001b[34mEpoch 34 - Training loss: 0.5330386548303068\u001b[0m\n",
      "\u001b[34mEpoch 35 - Training loss: 0.5344284197315574\u001b[0m\n",
      "\u001b[34mEpoch 36 - Training loss: 0.5438008923083544\u001b[0m\n",
      "\u001b[34mEpoch 37 - Training loss: 0.5411821189336479\u001b[0m\n",
      "\u001b[34mEpoch 38 - Training loss: 0.5521457050926983\u001b[0m\n",
      "\u001b[34mEpoch 39 - Training loss: 0.5382116680964828\u001b[0m\n",
      "\u001b[34mEpoch 40 - Training loss: 0.5483653764938936\u001b[0m\n",
      "\u001b[34mEpoch 41 - Training loss: 0.5503405773080885\u001b[0m\n",
      "\u001b[34mEpoch 42 - Training loss: 0.5453117058146745\u001b[0m\n",
      "\u001b[34mEpoch 43 - Training loss: 0.5470580744091421\u001b[0m\n",
      "\u001b[34mEpoch 44 - Training loss: 0.5514637153130024\u001b[0m\n",
      "\u001b[34mEpoch 45 - Training loss: 0.5358822224196047\u001b[0m\n",
      "\u001b[34mEpoch 46 - Training loss: 0.5355024707969278\u001b[0m\n",
      "\u001b[34mEpoch 47 - Training loss: 0.5500750448089093\u001b[0m\n",
      "\u001b[34mEpoch 48 - Training loss: 0.5463350976817309\u001b[0m\n",
      "\u001b[34mEpoch 49 - Training loss: 0.548837838601321\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/opacus/privacy_engine.py:760: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
      "  \"A ``sample_rate`` has been provided.\"\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/opacus/privacy_engine.py:237: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:796: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\u001b[0m\n",
      "\u001b[34m2022-02-28 20:33:49,210 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-02-28 20:34:03 Uploading - Uploading generated training model\n",
      "2022-02-28 20:34:03 Completed - Training job completed\n",
      "Training seconds: 441\n",
      "Billable seconds: 441\n"
     ]
    }
   ],
   "source": [
    "pytorch_estimator.fit({'train': 's3://responsibleai/data/churn.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.8-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
