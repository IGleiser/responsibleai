{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: codecarbon in /opt/conda/lib/python3.6/site-packages (1.2.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from codecarbon) (2.26.0)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.6/site-packages (from codecarbon) (8.0.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from codecarbon) (1.1.5)\n",
      "Requirement already satisfied: pynvml in /opt/conda/lib/python3.6/site-packages (from codecarbon) (11.4.1)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from codecarbon) (0.8)\n",
      "Requirement already satisfied: dash-bootstrap-components in /opt/conda/lib/python3.6/site-packages (from codecarbon) (1.0.3)\n",
      "Requirement already satisfied: dash in /opt/conda/lib/python3.6/site-packages (from codecarbon) (2.2.0)\n",
      "Requirement already satisfied: APScheduler in /opt/conda/lib/python3.6/site-packages (from codecarbon) (3.9.0.post1)\n",
      "Requirement already satisfied: fire in /opt/conda/lib/python3.6/site-packages (from codecarbon) (0.4.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.6/site-packages (from APScheduler->codecarbon) (1.16.0)\n",
      "Requirement already satisfied: setuptools>=0.7 in /opt/conda/lib/python3.6/site-packages (from APScheduler->codecarbon) (59.3.0)\n",
      "Requirement already satisfied: tzlocal!=3.*,>=2.0 in /opt/conda/lib/python3.6/site-packages (from APScheduler->codecarbon) (4.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from APScheduler->codecarbon) (2021.3)\n",
      "Requirement already satisfied: dash-html-components==2.0.0 in /opt/conda/lib/python3.6/site-packages (from dash->codecarbon) (2.0.0)\n",
      "Requirement already satisfied: dash-core-components==2.0.0 in /opt/conda/lib/python3.6/site-packages (from dash->codecarbon) (2.0.0)\n",
      "Requirement already satisfied: Flask>=1.0.4 in /opt/conda/lib/python3.6/site-packages (from dash->codecarbon) (2.0.2)\n",
      "Requirement already satisfied: flask-compress in /opt/conda/lib/python3.6/site-packages (from dash->codecarbon) (1.10.1)\n",
      "Requirement already satisfied: importlib-metadata==4.8.3 in /opt/conda/lib/python3.6/site-packages (from dash->codecarbon) (4.8.3)\n",
      "Requirement already satisfied: plotly>=5.0.0 in /opt/conda/lib/python3.6/site-packages (from dash->codecarbon) (5.4.0)\n",
      "Requirement already satisfied: dash-table==5.0.0 in /opt/conda/lib/python3.6/site-packages (from dash->codecarbon) (5.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata==4.8.3->dash->codecarbon) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata==4.8.3->dash->codecarbon) (3.10.0.2)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.6/site-packages (from fire->codecarbon) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->codecarbon) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.6/site-packages (from pandas->codecarbon) (1.19.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->codecarbon) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->codecarbon) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->codecarbon) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.6/site-packages (from requests->codecarbon) (2.0.4)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.6/site-packages (from Flask>=1.0.4->dash->codecarbon) (3.0.3)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /opt/conda/lib/python3.6/site-packages (from Flask>=1.0.4->dash->codecarbon) (2.0.1)\n",
      "Requirement already satisfied: Werkzeug>=2.0 in /opt/conda/lib/python3.6/site-packages (from Flask>=1.0.4->dash->codecarbon) (2.0.2)\n",
      "Requirement already satisfied: click>=7.1.2 in /opt/conda/lib/python3.6/site-packages (from Flask>=1.0.4->dash->codecarbon) (8.0.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.6/site-packages (from plotly>=5.0.0->dash->codecarbon) (8.0.1)\n",
      "Requirement already satisfied: backports.zoneinfo in /opt/conda/lib/python3.6/site-packages (from tzlocal!=3.*,>=2.0->APScheduler->codecarbon) (0.2.1)\n",
      "Requirement already satisfied: pytz-deprecation-shim in /opt/conda/lib/python3.6/site-packages (from tzlocal!=3.*,>=2.0->APScheduler->codecarbon) (0.1.0.post0)\n",
      "Requirement already satisfied: brotli in /opt/conda/lib/python3.6/site-packages (from flask-compress->dash->codecarbon) (1.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.6/site-packages (from Jinja2>=3.0->Flask>=1.0.4->dash->codecarbon) (2.0.1)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.6/site-packages (from backports.zoneinfo->tzlocal!=3.*,>=2.0->APScheduler->codecarbon) (5.4.0)\n",
      "Requirement already satisfied: tzdata in /opt/conda/lib/python3.6/site-packages (from pytz-deprecation-shim->tzlocal!=3.*,>=2.0->APScheduler->codecarbon) (2021.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install codecarbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#library for carbon emission\n",
    "from codecarbon import EmissionsTracker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChurnDataset(Dataset):\n",
    " \n",
    "    def __init__(self, csv_file):\n",
    "  \n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        df = df.drop([\"Surname\", \"CustomerId\", \"RowNumber\"], axis=1)\n",
    "\n",
    "        # Grouping variable names\n",
    "        self.categorical = [\"Geography\", \"Gender\"]\n",
    "        self.target = \"Exited\"\n",
    "\n",
    "        # One-hot encoding of categorical variables\n",
    "        self.churn_frame = pd.get_dummies(df, prefix=self.categorical)\n",
    "\n",
    "        # Save target and predictors\n",
    "        self.X = self.churn_frame.drop(self.target, axis=1)\n",
    "        self.y = self.churn_frame[\"Exited\"]\n",
    "        \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_array  = scaler.fit_transform(self.X)\n",
    "        self.X = pd.DataFrame(X_array)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.churn_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert idx from tensor to list due to pandas bug (that arises when using pytorch's random_split)\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        return [self.X.iloc[idx].values, self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CHURN_model():\n",
    "    model = nn.Sequential(nn.Linear(13, 64), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(64, 64), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(64, 1)) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(csv_file, batch_size):\n",
    "     # Load dataset\n",
    "    dataset = ChurnDataset(csv_file)\n",
    "\n",
    "    # Split into training and test\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    trainset, testset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return trainloader, testloader, trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, net, optimizer, n_epochs=100):\n",
    "     \n",
    "    device = \"cpu\"\n",
    "\n",
    "    # Define the model\n",
    "    #net = get_CHURN_model()\n",
    "    net = net.to(device)\n",
    "    \n",
    "    #criterion = nn.CrossEntropyLoss() \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "    # Train the net\n",
    "    loss_per_iter = []\n",
    "    loss_per_batch = []\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward + backward + optimize\n",
    "            outputs = net(inputs.float())\n",
    "            loss = criterion(outputs, labels.float().unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Save loss to plot\n",
    "            running_loss += loss.item()\n",
    "            loss_per_iter.append(loss.item())\n",
    "\n",
    "        \n",
    "        print(\"Epoch {} - Training loss: {}\".format(epoch, running_loss/len(trainloader))) \n",
    "        \n",
    "        running_loss = 0.0\n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"../data/churn.csv\"\n",
    "\n",
    "trainloader, testloader, train_ds, test_ds = get_dataloader(csv_file, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CODECARBON : No CPU tracking mode found. Falling back on CPU constant mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-02-26 21:00:05.492 pytorch-1-8-gpu-py3-ml-g4dn-xlarge-60bd0d07a83be181dcf7335baae2:1104 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-02-26 21:00:05.526 pytorch-1-8-gpu-py3-ml-g4dn-xlarge-60bd0d07a83be181dcf7335baae2:1104 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Epoch 0 - Training loss: 0.42982916031032803\n",
      "Epoch 1 - Training loss: 0.3540937858633697\n",
      "Epoch 2 - Training loss: 0.34025645079091194\n",
      "Epoch 3 - Training loss: 0.3379934617318213\n",
      "Epoch 4 - Training loss: 0.3337463465519249\n",
      "Epoch 5 - Training loss: 0.33179530426859855\n",
      "Epoch 6 - Training loss: 0.32722213892266155\n",
      "Epoch 7 - Training loss: 0.3252848910167813\n",
      "Epoch 8 - Training loss: 0.32446048059500754\n",
      "Epoch 9 - Training loss: 0.3215969825163484\n",
      "Epoch 10 - Training loss: 0.32005199883133173\n",
      "Epoch 11 - Training loss: 0.31845565321855246\n",
      "Epoch 12 - Training loss: 0.31571509074419735\n",
      "Epoch 13 - Training loss: 0.3144164187833667\n",
      "Epoch 14 - Training loss: 0.3135476074181497\n",
      "Epoch 15 - Training loss: 0.31294965390115975\n",
      "Epoch 16 - Training loss: 0.31114111226052044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:apscheduler.executors.default:Running job \"BaseEmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2022-02-26 21:00:20 UTC)\" (scheduled at 2022-02-26 21:00:20.231885+00:00)\n",
      "INFO:apscheduler.executors.default:Job \"BaseEmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2022-02-26 21:00:35 UTC)\" executed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Training loss: 0.308766683889553\n",
      "Epoch 18 - Training loss: 0.308532061195001\n",
      "Epoch 19 - Training loss: 0.3065998086705804\n",
      "Epoch 20 - Training loss: 0.3096625545993447\n",
      "Epoch 21 - Training loss: 0.304538294672966\n",
      "Epoch 22 - Training loss: 0.30369578558020294\n",
      "Epoch 23 - Training loss: 0.30321342647075655\n",
      "Epoch 24 - Training loss: 0.3018710108473897\n",
      "Epoch 25 - Training loss: 0.30227746022865176\n",
      "Epoch 26 - Training loss: 0.29813844375312326\n",
      "Epoch 27 - Training loss: 0.296247317455709\n",
      "Epoch 28 - Training loss: 0.29577001137658954\n",
      "Epoch 29 - Training loss: 0.2961962307803333\n",
      "Epoch 30 - Training loss: 0.29479620046913624\n",
      "Epoch 31 - Training loss: 0.2930365629028529\n",
      "Epoch 32 - Training loss: 0.2904843362979591\n",
      "Epoch 33 - Training loss: 0.2911228437907994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:apscheduler.executors.default:Running job \"BaseEmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2022-02-26 21:00:35 UTC)\" (scheduled at 2022-02-26 21:00:35.231885+00:00)\n",
      "INFO:apscheduler.executors.default:Job \"BaseEmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2022-02-26 21:00:50 UTC)\" executed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 - Training loss: 0.2866478441283107\n",
      "Epoch 35 - Training loss: 0.28569351113401353\n",
      "Epoch 36 - Training loss: 0.28702406734228136\n",
      "Epoch 37 - Training loss: 0.28462868379428985\n",
      "Epoch 38 - Training loss: 0.2842296312563121\n",
      "Epoch 39 - Training loss: 0.28239488578401506\n",
      "Epoch 40 - Training loss: 0.2809695449657738\n",
      "Epoch 41 - Training loss: 0.278782249847427\n",
      "Epoch 42 - Training loss: 0.2764643343165517\n",
      "Epoch 43 - Training loss: 0.2761734708212316\n",
      "Epoch 44 - Training loss: 0.2758705934742466\n",
      "Epoch 45 - Training loss: 0.2737947876099497\n",
      "Epoch 46 - Training loss: 0.2716989791020751\n",
      "Epoch 47 - Training loss: 0.2698657149448991\n",
      "Epoch 48 - Training loss: 0.2708817786537111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:apscheduler.scheduler:Scheduler has been shut down\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - Training loss: 0.26901235966943204\n",
      "0.0005244343289492949\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tracker = EmissionsTracker(project_name = \"churn_prediction\",\n",
    "                           output_dir = \"../output/\",\n",
    "                           measure_power_secs = 15,\n",
    "                           save_to_file = True)\n",
    "\n",
    "tracker.start()\n",
    "\n",
    "net = get_CHURN_model()\n",
    "optimizer = optim.Adam(net.parameters(), weight_decay=0.0001, lr=0.003)\n",
    "model = train(trainloader, net, optimizer, 50)\n",
    "\n",
    "emissions: float = tracker.stop()\n",
    "print(emissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.8-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
