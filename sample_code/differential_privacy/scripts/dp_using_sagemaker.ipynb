{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uri = S3Uploader.upload(\"../data/churn.csv\", 's3://responsibleai/data')\n",
    "output_path = 's3://responsibleai/dp/output'\n",
    "checkpoint_local_path=\"/opt/ml/checkpoints\"\n",
    "checkpoint_s3_bucket = 's3://responsibleai/dp/checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_estimator = PyTorch(entry_point = '../code/train.py',\n",
    "                            source_dir = '../code',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            output_path = 's3://responsibleai/dp/output',\n",
    "                            checkpoint_s3_uri=checkpoint_s3_bucket,\n",
    "                            checkpoint_local_path=checkpoint_local_path,\n",
    "                            role = role,\n",
    "                            instance_count=1,\n",
    "                            framework_version='1.8.0',\n",
    "                            py_version='py3',\n",
    "                            hyperparameters = {'epochs': 100, \n",
    "                                               'batch-size': 64, \n",
    "                                               'learning-rate': 0.003})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-02 22:49:34 Starting - Starting the training job...ProfilerReport-1646261374: InProgress\n",
      "...\n",
      "2022-03-02 22:50:31 Starting - Preparing the instances for training......\n",
      "2022-03-02 22:51:33 Downloading - Downloading input data...\n",
      "2022-03-02 22:51:58 Training - Downloading the training image...........................\n",
      "2022-03-02 22:56:26 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-03-02 22:56:29,763 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-03-02 22:56:29,789 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-03-02 22:56:29,797 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-03-02 22:56:30,779 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting opacus\n",
      "  Downloading opacus-0.15.0-py3-none-any.whl (125 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.6/site-packages (from opacus->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from opacus->-r requirements.txt (line 1)) (1.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.2 in /opt/conda/lib/python3.6/site-packages (from opacus->-r requirements.txt (line 1)) (1.5.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch>=1.3->opacus->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3->opacus->-r requirements.txt (line 1)) (3.7.4.3)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: opacus\u001b[0m\n",
      "\u001b[34mSuccessfully installed opacus-0.15.0\u001b[0m\n",
      "\u001b[34m2022-03-02 22:56:33,247 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 64,\n",
      "        \"epochs\": 100,\n",
      "        \"learning-rate\": 0.003\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-03-02-22-49-34-337\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://responsibleai/pytorch-training-2022-03-02-22-49-34-337/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"../code/train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"../code/train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":64,\"epochs\":100,\"learning-rate\":0.003}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=../code/train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=../code/train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://responsibleai/pytorch-training-2022-03-02-22-49-34-337/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":64,\"epochs\":100,\"learning-rate\":0.003},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-03-02-22-49-34-337\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://responsibleai/pytorch-training-2022-03-02-22-49-34-337/source/sourcedir.tar.gz\",\"module_name\":\"../code/train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"../code/train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"64\",\"--epochs\",\"100\",\"--learning-rate\",\"0.003\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=100\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.003\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 ../code/train.py --batch-size 64 --epochs 100 --learning-rate 0.003\u001b[0m\n",
      "\u001b[34mNamespace(current_host='algo-1', hosts=['algo-1'], model_dir='/opt/ml/model', output='/opt/ml/output', output_data_dir='/opt/ml/output/data', sm_model_dir='/opt/ml/model', train='/opt/ml/input/data/train')\u001b[0m\n",
      "\u001b[34mEpoch 0 - Training loss: 0.441622631624341\u001b[0m\n",
      "\u001b[34mEpoch 1 - Training loss: 0.3655851410701871\u001b[0m\n",
      "\u001b[34mEpoch 2 - Training loss: 0.3499747648835182\u001b[0m\n",
      "\u001b[34mEpoch 3 - Training loss: 0.34485527966171503\u001b[0m\n",
      "\u001b[34mEpoch 4 - Training loss: 0.34021206982433794\u001b[0m\n",
      "\u001b[34mEpoch 5 - Training loss: 0.33743663048371675\u001b[0m\n",
      "\u001b[34mEpoch 6 - Training loss: 0.33774746106937525\u001b[0m\n",
      "\u001b[34mEpoch 7 - Training loss: 0.33393034506589175\u001b[0m\n",
      "\u001b[34mEpoch 8 - Training loss: 0.33049120008945465\u001b[0m\n",
      "\u001b[34mEpoch 9 - Training loss: 0.3294257266446948\u001b[0m\n",
      "\u001b[34mEpoch 10 - Training loss: 0.32555110771209\u001b[0m\n",
      "\u001b[34mEpoch 11 - Training loss: 0.327008708473295\u001b[0m\n",
      "\u001b[34mEpoch 12 - Training loss: 0.32371517876163125\u001b[0m\n",
      "\u001b[34mEpoch 13 - Training loss: 0.323500276915729\u001b[0m\n",
      "\u001b[34mEpoch 14 - Training loss: 0.3227882781997323\u001b[0m\n",
      "\u001b[34mEpoch 15 - Training loss: 0.319215558283031\u001b[0m\n",
      "\u001b[34mEpoch 16 - Training loss: 0.3191146368160844\u001b[0m\n",
      "\u001b[34mEpoch 17 - Training loss: 0.3175011773593724\u001b[0m\n",
      "\u001b[34mEpoch 18 - Training loss: 0.31424200870096686\u001b[0m\n",
      "\u001b[34mEpoch 19 - Training loss: 0.3130408347584307\u001b[0m\n",
      "\u001b[34mEpoch 20 - Training loss: 0.31304266219958665\u001b[0m\n",
      "\u001b[34mEpoch 21 - Training loss: 0.3106203810311854\u001b[0m\n",
      "\u001b[34mEpoch 22 - Training loss: 0.31040060380473733\u001b[0m\n",
      "\u001b[34mEpoch 23 - Training loss: 0.30971506526693704\u001b[0m\n",
      "\u001b[34mEpoch 24 - Training loss: 0.3072472808882594\u001b[0m\n",
      "\u001b[34mEpoch 25 - Training loss: 0.30664765681140127\u001b[0m\n",
      "\u001b[34mEpoch 26 - Training loss: 0.30682034278288484\u001b[0m\n",
      "\u001b[34mEpoch 27 - Training loss: 0.3027484524063766\u001b[0m\n",
      "\u001b[34mEpoch 28 - Training loss: 0.3031912879087031\u001b[0m\n",
      "\u001b[34mEpoch 29 - Training loss: 0.299130393890664\u001b[0m\n",
      "\u001b[34mEpoch 30 - Training loss: 0.2985411973670125\u001b[0m\n",
      "\u001b[34mEpoch 31 - Training loss: 0.2962395393289626\u001b[0m\n",
      "\u001b[34mEpoch 32 - Training loss: 0.2960101327858865\u001b[0m\n",
      "\u001b[34mEpoch 33 - Training loss: 0.2939647781196982\u001b[0m\n",
      "\u001b[34mEpoch 34 - Training loss: 0.2931136996485293\u001b[0m\n",
      "\u001b[34mEpoch 35 - Training loss: 0.2927399821579456\u001b[0m\n",
      "\u001b[34mEpoch 36 - Training loss: 0.288793221116066\u001b[0m\n",
      "\u001b[34mEpoch 37 - Training loss: 0.28811212722212076\u001b[0m\n",
      "\u001b[34mEpoch 38 - Training loss: 0.2895585823338479\u001b[0m\n",
      "\u001b[34mEpoch 39 - Training loss: 0.28617598111741244\u001b[0m\n",
      "\u001b[34mEpoch 40 - Training loss: 0.2838259247597307\u001b[0m\n",
      "\u001b[34mEpoch 41 - Training loss: 0.2824718096293509\u001b[0m\n",
      "\u001b[34mEpoch 42 - Training loss: 0.28039726717397573\u001b[0m\n",
      "\u001b[34mEpoch 43 - Training loss: 0.2796290358994156\u001b[0m\n",
      "\u001b[34mEpoch 44 - Training loss: 0.27718675378710034\u001b[0m\n",
      "\u001b[34mEpoch 45 - Training loss: 0.27519186222925784\u001b[0m\n",
      "\u001b[34mEpoch 46 - Training loss: 0.27386135412380097\u001b[0m\n",
      "\u001b[34mEpoch 47 - Training loss: 0.2718489187303931\u001b[0m\n",
      "\u001b[34mEpoch 48 - Training loss: 0.26980824964120986\u001b[0m\n",
      "\u001b[34mEpoch 49 - Training loss: 0.26672647735103966\u001b[0m\n",
      "\u001b[34m####------- Model training with Differential Privacy -------####\u001b[0m\n",
      "\u001b[34mEpoch 0 - Training loss: 0.5569128097034991\u001b[0m\n",
      "\u001b[34mEpoch 1 - Training loss: 0.5361758138984442\u001b[0m\n",
      "\u001b[34mEpoch 2 - Training loss: 0.5258792238309979\u001b[0m\n",
      "\u001b[34mEpoch 3 - Training loss: 0.5278338454663754\u001b[0m\n",
      "\u001b[34mEpoch 4 - Training loss: 0.5296237818896771\u001b[0m\n",
      "\u001b[34mEpoch 5 - Training loss: 0.5313609718345106\u001b[0m\n",
      "\u001b[34mEpoch 6 - Training loss: 0.5281738995108753\u001b[0m\n",
      "\u001b[34mEpoch 7 - Training loss: 0.5235448802355677\u001b[0m\n",
      "\u001b[34mEpoch 8 - Training loss: 0.52565943216905\u001b[0m\n",
      "\u001b[34mEpoch 9 - Training loss: 0.5228037925437092\u001b[0m\n",
      "\u001b[34mEpoch 10 - Training loss: 0.5218283424153924\u001b[0m\n",
      "\u001b[34mEpoch 11 - Training loss: 0.5241514152847231\u001b[0m\n",
      "\u001b[34mEpoch 12 - Training loss: 0.5143364879302681\u001b[0m\n",
      "\u001b[34mEpoch 13 - Training loss: 0.5033696616068483\u001b[0m\n",
      "\u001b[34mEpoch 14 - Training loss: 0.5059597201645374\u001b[0m\n",
      "\u001b[34mEpoch 15 - Training loss: 0.5099008854478597\u001b[0m\n",
      "\u001b[34mEpoch 16 - Training loss: 0.5107007127255201\u001b[0m\n",
      "\u001b[34mEpoch 17 - Training loss: 0.512005403637886\u001b[0m\n",
      "\u001b[34mEpoch 18 - Training loss: 0.5115557321812958\u001b[0m\n",
      "\u001b[34mEpoch 19 - Training loss: 0.5096040079370141\u001b[0m\n",
      "\u001b[34mEpoch 20 - Training loss: 0.5119723560288548\u001b[0m\n",
      "\u001b[34mEpoch 21 - Training loss: 0.5155564672313631\u001b[0m\n",
      "\u001b[34mEpoch 22 - Training loss: 0.5144468046724796\u001b[0m\n",
      "\u001b[34mEpoch 23 - Training loss: 0.5144044117070734\u001b[0m\n",
      "\u001b[34mEpoch 24 - Training loss: 0.5146601868327707\u001b[0m\n",
      "\u001b[34mEpoch 25 - Training loss: 0.5190848752856254\u001b[0m\n",
      "\u001b[34mEpoch 26 - Training loss: 0.5212190198712051\u001b[0m\n",
      "\u001b[34mEpoch 27 - Training loss: 0.5309244010131806\u001b[0m\n",
      "\u001b[34mEpoch 28 - Training loss: 0.5301303688902408\u001b[0m\n",
      "\u001b[34mEpoch 29 - Training loss: 0.5229427948594093\u001b[0m\n",
      "\u001b[34mEpoch 30 - Training loss: 0.518906374135986\u001b[0m\n",
      "\u001b[34mEpoch 31 - Training loss: 0.5189341157674789\u001b[0m\n",
      "\u001b[34mEpoch 32 - Training loss: 0.523358553647995\u001b[0m\n",
      "\u001b[34mEpoch 33 - Training loss: 0.5212080560042522\u001b[0m\n",
      "\u001b[34mEpoch 34 - Training loss: 0.5061094858683646\u001b[0m\n",
      "\u001b[34mEpoch 35 - Training loss: 0.5132642779732123\u001b[0m\n",
      "\u001b[34mEpoch 36 - Training loss: 0.5139302023686468\u001b[0m\n",
      "\u001b[34mEpoch 37 - Training loss: 0.5161555403843522\u001b[0m\n",
      "\u001b[34mEpoch 38 - Training loss: 0.517663206718862\u001b[0m\n",
      "\u001b[34mEpoch 39 - Training loss: 0.5292435147799551\u001b[0m\n",
      "\u001b[34mEpoch 40 - Training loss: 0.5257889630272985\u001b[0m\n",
      "\u001b[34mEpoch 41 - Training loss: 0.5453623522538692\u001b[0m\n",
      "\u001b[34mEpoch 42 - Training loss: 0.5375750477425754\u001b[0m\n",
      "\u001b[34mEpoch 43 - Training loss: 0.5319883599877357\u001b[0m\n",
      "\u001b[34mEpoch 44 - Training loss: 0.5204654054250568\u001b[0m\n",
      "\u001b[34mEpoch 45 - Training loss: 0.5268318598624319\u001b[0m\n",
      "\u001b[34mEpoch 46 - Training loss: 0.5261120873503387\u001b[0m\n",
      "\u001b[34mEpoch 47 - Training loss: 0.5323375607840717\u001b[0m\n",
      "\u001b[34mEpoch 48 - Training loss: 0.5252577485051007\u001b[0m\n",
      "\u001b[34mEpoch 49 - Training loss: 0.5259578396566212\n",
      " ε = 6.39, δ = 1e-06\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/opacus/privacy_engine.py:760: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
      "  \"A ``sample_rate`` has been provided.\"\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/opacus/privacy_engine.py:237: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:796: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\u001b[0m\n",
      "\u001b[34m2022-03-02 22:59:18,894 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-03-02 22:59:41 Uploading - Uploading generated training modelProfilerReport-1646261374: NoIssuesFound\n",
      "\n",
      "2022-03-02 23:00:06 Completed - Training job completed\n",
      "Training seconds: 513\n",
      "Billable seconds: 513\n"
     ]
    }
   ],
   "source": [
    "pytorch_estimator.fit({'train': 's3://responsibleai/data/churn.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: churn-clarify-model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    }
   ],
   "source": [
    "model_name = \"churn-clarify-model\" \n",
    "\n",
    "predictor = pytorch_estimator.deploy(initial_instance_count=1, \n",
    "                         instance_type='ml.p3.2xlarge',\n",
    "                         model_name = \"churn-clarify-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.8-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
