{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opacus\n",
      "  Using cached opacus-0.15.0-py3-none-any.whl (125 kB)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.6/site-packages (from opacus) (1.19.1)\n",
      "Requirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from opacus) (1.8.1)\n",
      "Requirement already satisfied: scipy>=1.2 in /opt/conda/lib/python3.6/site-packages (from opacus) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3->opacus) (3.10.0.2)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch>=1.3->opacus) (0.8)\n",
      "Installing collected packages: opacus\n",
      "Successfully installed opacus-0.15.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opacus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting codecarbon\n",
      "  Using cached codecarbon-1.2.0-py3-none-any.whl (135 kB)\n",
      "Collecting py-cpuinfo\n",
      "  Using cached py_cpuinfo-8.0.0-py3-none-any.whl\n",
      "Collecting dash\n",
      "  Using cached dash-2.2.0-py3-none-any.whl (8.5 MB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from codecarbon) (2.26.0)\n",
      "Collecting dash-bootstrap-components\n",
      "  Using cached dash_bootstrap_components-1.0.3-py3-none-any.whl (209 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from codecarbon) (1.1.5)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from codecarbon) (0.8)\n",
      "Collecting pynvml\n",
      "  Using cached pynvml-11.4.1-py3-none-any.whl (46 kB)\n",
      "Collecting APScheduler\n",
      "  Downloading APScheduler-3.9.1-py2.py3-none-any.whl (59 kB)\n",
      "     |████████████████████████████████| 59 kB 1.5 MB/s             \n",
      "\u001b[?25hCollecting fire\n",
      "  Using cached fire-0.4.0-py2.py3-none-any.whl\n",
      "Collecting tzlocal!=3.*,>=2.0\n",
      "  Using cached tzlocal-4.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.6/site-packages (from APScheduler->codecarbon) (1.16.0)\n",
      "Requirement already satisfied: setuptools>=0.7 in /opt/conda/lib/python3.6/site-packages (from APScheduler->codecarbon) (59.3.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from APScheduler->codecarbon) (2021.3)\n",
      "Collecting dash-table==5.0.0\n",
      "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
      "Requirement already satisfied: Flask>=1.0.4 in /opt/conda/lib/python3.6/site-packages (from dash->codecarbon) (2.0.2)\n",
      "Collecting flask-compress\n",
      "  Downloading Flask_Compress-1.11-py3-none-any.whl (7.9 kB)\n",
      "Collecting dash-core-components==2.0.0\n",
      "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
      "Requirement already satisfied: importlib-metadata==4.8.3 in /opt/conda/lib/python3.6/site-packages (from dash->codecarbon) (4.8.3)\n",
      "Collecting dash-html-components==2.0.0\n",
      "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
      "Requirement already satisfied: plotly>=5.0.0 in /opt/conda/lib/python3.6/site-packages (from dash->codecarbon) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata==4.8.3->dash->codecarbon) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata==4.8.3->dash->codecarbon) (3.6.0)\n",
      "Collecting termcolor\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->codecarbon) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.6/site-packages (from pandas->codecarbon) (1.19.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->codecarbon) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.6/site-packages (from requests->codecarbon) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->codecarbon) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->codecarbon) (2.10)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.6/site-packages (from Flask>=1.0.4->dash->codecarbon) (3.0.3)\n",
      "Requirement already satisfied: click>=7.1.2 in /opt/conda/lib/python3.6/site-packages (from Flask>=1.0.4->dash->codecarbon) (8.0.3)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /opt/conda/lib/python3.6/site-packages (from Flask>=1.0.4->dash->codecarbon) (2.0.1)\n",
      "Requirement already satisfied: Werkzeug>=2.0 in /opt/conda/lib/python3.6/site-packages (from Flask>=1.0.4->dash->codecarbon) (2.0.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.6/site-packages (from plotly>=5.0.0->dash->codecarbon) (8.0.1)\n",
      "Collecting pytz-deprecation-shim\n",
      "  Using cached pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting backports.zoneinfo\n",
      "  Using cached backports.zoneinfo-0.2.1-cp36-cp36m-manylinux1_x86_64.whl (70 kB)\n",
      "Collecting brotli\n",
      "  Using cached Brotli-1.0.9-cp36-cp36m-manylinux1_x86_64.whl (357 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.6/site-packages (from Jinja2>=3.0->Flask>=1.0.4->dash->codecarbon) (2.0.1)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.6/site-packages (from backports.zoneinfo->tzlocal!=3.*,>=2.0->APScheduler->codecarbon) (5.4.0)\n",
      "Collecting tzdata\n",
      "  Using cached tzdata-2021.5-py2.py3-none-any.whl (339 kB)\n",
      "Installing collected packages: tzdata, brotli, backports.zoneinfo, pytz-deprecation-shim, flask-compress, dash-table, dash-html-components, dash-core-components, tzlocal, termcolor, dash, pynvml, py-cpuinfo, fire, dash-bootstrap-components, APScheduler, codecarbon\n",
      "Successfully installed APScheduler-3.9.1 backports.zoneinfo-0.2.1 brotli-1.0.9 codecarbon-1.2.0 dash-2.2.0 dash-bootstrap-components-1.0.3 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 fire-0.4.0 flask-compress-1.11 py-cpuinfo-8.0.0 pynvml-11.4.1 pytz-deprecation-shim-0.1.0.post0 termcolor-1.1.0 tzdata-2021.5 tzlocal-4.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install codecarbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting captum\n",
      "  Downloading captum-0.5.0-py3-none-any.whl (1.4 MB)\n",
      "     |████████████████████████████████| 1.4 MB 24.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from captum) (3.3.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from captum) (1.19.1)\n",
      "Requirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.6/site-packages (from captum) (1.8.1)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch>=1.6->captum) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.6->captum) (3.10.0.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->captum) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.6/site-packages (from matplotlib->captum) (3.0.6)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.6/site-packages (from matplotlib->captum) (8.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->captum) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->captum) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib->captum) (1.16.0)\n",
      "Installing collected packages: captum\n",
      "Successfully installed captum-0.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from codecarbon import EmissionsTracker as ET\n",
    "from opacus import PrivacyEngine\n",
    "\n",
    "from captum.attr import IntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  Pclass   Age  SibSp  Parch     Fare  Sex_female  Sex_male  \\\n",
       "0       0       3  22.0      1      0   7.2500           0         1   \n",
       "1       1       1  38.0      1      0  71.2833           1         0   \n",
       "2       1       3  26.0      0      0   7.9250           1         0   \n",
       "3       1       1  35.0      1      0  53.1000           1         0   \n",
       "4       0       3  35.0      0      0   8.0500           0         1   \n",
       "\n",
       "   Embarked_C  Embarked_Q  Embarked_S  \n",
       "0           0           0           1  \n",
       "1           1           0           0  \n",
       "2           0           0           1  \n",
       "3           0           0           1  \n",
       "4           0           0           1  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from csv file\n",
    "df = pd.read_csv('../data/titanic.csv')\n",
    "df.drop(['PassengerId', 'Name'], axis=1, inplace=True)\n",
    "\n",
    "categorical_columns = ['Sex', 'Embarked']\n",
    "df_cleaned = pd.get_dummies(df, prefix=categorical_columns)\n",
    "df_cleaned['Age'].fillna(inplace= True, method='bfill' )\n",
    "\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target        0\n",
       "Pclass        0\n",
       "Age           0\n",
       "SibSp         0\n",
       "Parch         0\n",
       "Fare          0\n",
       "Sex_female    0\n",
       "Sex_male      0\n",
       "Embarked_C    0\n",
       "Embarked_Q    0\n",
       "Embarked_S    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting dataframe to numpy array\n",
    "labels = df_cleaned[\"target\"].to_numpy()\n",
    "\n",
    "df_cleaned = df_cleaned.drop(['target'], axis=1)\n",
    "feature_names = list(df_cleaned.columns)\n",
    "features = df_cleaned.to_numpy()\n",
    "\n",
    "# loading data into torch tensor\n",
    "feature_tensor = torch.from_numpy(features).type(torch.FloatTensor)\n",
    "label_tensor = torch.from_numpy(labels)\n",
    "\n",
    "# loading data into torch dataset\n",
    "train_dataset = torch.utils.data.TensorDataset(feature_tensor, label_tensor)\n",
    "\n",
    "# loading data into torch dataloader\n",
    "batch_size = 32\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([891, 10])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(10, 10)\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        self.linear2 = nn.Linear(10, 8)\n",
    "        self.sigmoid2 = nn.Sigmoid()\n",
    "        self.linear3 = nn.Linear(8, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lin1_out = self.linear1(x)\n",
    "        sigmoid_out1 = self.sigmoid1(lin1_out)\n",
    "        sigmoid_out2 = self.sigmoid2(self.linear2(sigmoid_out1))\n",
    "        return self.softmax(self.linear3(sigmoid_out2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, model, optimizer):\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    epochs = 100\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, (features, label) in enumerate(train_dataloader):\n",
    "            # Forward pass\n",
    "            y_pred = model(features)\n",
    "            loss = loss_fn(y_pred, label)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 10 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch+1, epochs, i+1, len(train_dataloader), loss.item()))\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:codecarbon.emissions_tracker:CODECARBON : No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "INFO:apscheduler.scheduler:Adding job tentatively -- it will be properly scheduled when the scheduler starts\n",
      "INFO:apscheduler.scheduler:Added job \"BaseEmissionsTracker._measure_power\" to job store \"default\"\n",
      "INFO:apscheduler.scheduler:Scheduler started\n"
     ]
    }
   ],
   "source": [
    "# Calculate Emissions\n",
    "tracker = ET(project_name = \"churn_prediction\",\n",
    "                           output_dir = \"../output/\",\n",
    "                           measure_power_secs = 15,\n",
    "                           save_to_file = True)\n",
    "\n",
    "tracker.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [10/28], Loss: 0.6830\n",
      "Epoch [1/100], Step [20/28], Loss: 0.6740\n",
      "Epoch [2/100], Step [10/28], Loss: 0.6781\n",
      "Epoch [2/100], Step [20/28], Loss: 0.6528\n",
      "Epoch [3/100], Step [10/28], Loss: 0.6915\n",
      "Epoch [3/100], Step [20/28], Loss: 0.6240\n",
      "Epoch [4/100], Step [10/28], Loss: 0.6439\n",
      "Epoch [4/100], Step [20/28], Loss: 0.6390\n",
      "Epoch [5/100], Step [10/28], Loss: 0.6381\n",
      "Epoch [5/100], Step [20/28], Loss: 0.6579\n",
      "Epoch [6/100], Step [10/28], Loss: 0.6706\n",
      "Epoch [6/100], Step [20/28], Loss: 0.6546\n",
      "Epoch [7/100], Step [10/28], Loss: 0.6129\n",
      "Epoch [7/100], Step [20/28], Loss: 0.6607\n",
      "Epoch [8/100], Step [10/28], Loss: 0.7060\n",
      "Epoch [8/100], Step [20/28], Loss: 0.6877\n",
      "Epoch [9/100], Step [10/28], Loss: 0.6790\n",
      "Epoch [9/100], Step [20/28], Loss: 0.5776\n",
      "Epoch [10/100], Step [10/28], Loss: 0.7145\n",
      "Epoch [10/100], Step [20/28], Loss: 0.6667\n",
      "Epoch [11/100], Step [10/28], Loss: 0.6124\n",
      "Epoch [11/100], Step [20/28], Loss: 0.6373\n",
      "Epoch [12/100], Step [10/28], Loss: 0.6713\n",
      "Epoch [12/100], Step [20/28], Loss: 0.7082\n",
      "Epoch [13/100], Step [10/28], Loss: 0.5925\n",
      "Epoch [13/100], Step [20/28], Loss: 0.6739\n",
      "Epoch [14/100], Step [10/28], Loss: 0.6525\n",
      "Epoch [14/100], Step [20/28], Loss: 0.6843\n",
      "Epoch [15/100], Step [10/28], Loss: 0.6159\n",
      "Epoch [15/100], Step [20/28], Loss: 0.6674\n",
      "Epoch [16/100], Step [10/28], Loss: 0.6847\n",
      "Epoch [16/100], Step [20/28], Loss: 0.5609\n",
      "Epoch [17/100], Step [10/28], Loss: 0.6609\n",
      "Epoch [17/100], Step [20/28], Loss: 0.5959\n",
      "Epoch [18/100], Step [10/28], Loss: 0.6614\n",
      "Epoch [18/100], Step [20/28], Loss: 0.6827\n",
      "Epoch [19/100], Step [10/28], Loss: 0.6187\n",
      "Epoch [19/100], Step [20/28], Loss: 0.5757\n",
      "Epoch [20/100], Step [10/28], Loss: 0.6183\n",
      "Epoch [20/100], Step [20/28], Loss: 0.6268\n",
      "Epoch [21/100], Step [10/28], Loss: 0.6550\n",
      "Epoch [21/100], Step [20/28], Loss: 0.5767\n",
      "Epoch [22/100], Step [10/28], Loss: 0.6567\n",
      "Epoch [22/100], Step [20/28], Loss: 0.6374\n",
      "Epoch [23/100], Step [10/28], Loss: 0.6081\n",
      "Epoch [23/100], Step [20/28], Loss: 0.6102\n",
      "Epoch [24/100], Step [10/28], Loss: 0.6101\n",
      "Epoch [24/100], Step [20/28], Loss: 0.6688\n",
      "Epoch [25/100], Step [10/28], Loss: 0.6516\n",
      "Epoch [25/100], Step [20/28], Loss: 0.6068\n",
      "Epoch [26/100], Step [10/28], Loss: 0.6081\n",
      "Epoch [26/100], Step [20/28], Loss: 0.5198\n",
      "Epoch [27/100], Step [10/28], Loss: 0.5470\n",
      "Epoch [27/100], Step [20/28], Loss: 0.6094\n",
      "Epoch [28/100], Step [10/28], Loss: 0.5315\n",
      "Epoch [28/100], Step [20/28], Loss: 0.6138\n",
      "Epoch [29/100], Step [10/28], Loss: 0.6119\n",
      "Epoch [29/100], Step [20/28], Loss: 0.5175\n",
      "Epoch [30/100], Step [10/28], Loss: 0.5995\n",
      "Epoch [30/100], Step [20/28], Loss: 0.5355\n",
      "Epoch [31/100], Step [10/28], Loss: 0.5556\n",
      "Epoch [31/100], Step [20/28], Loss: 0.5812\n",
      "Epoch [32/100], Step [10/28], Loss: 0.4353\n",
      "Epoch [32/100], Step [20/28], Loss: 0.4980\n",
      "Epoch [33/100], Step [10/28], Loss: 0.4951\n",
      "Epoch [33/100], Step [20/28], Loss: 0.5767\n",
      "Epoch [34/100], Step [10/28], Loss: 0.5053\n",
      "Epoch [34/100], Step [20/28], Loss: 0.5359\n",
      "Epoch [35/100], Step [10/28], Loss: 0.6139\n",
      "Epoch [35/100], Step [20/28], Loss: 0.4982\n",
      "Epoch [36/100], Step [10/28], Loss: 0.5543\n",
      "Epoch [36/100], Step [20/28], Loss: 0.4958\n",
      "Epoch [37/100], Step [10/28], Loss: 0.4861\n",
      "Epoch [37/100], Step [20/28], Loss: 0.5194\n",
      "Epoch [38/100], Step [10/28], Loss: 0.4398\n",
      "Epoch [38/100], Step [20/28], Loss: 0.4644\n",
      "Epoch [39/100], Step [10/28], Loss: 0.6009\n",
      "Epoch [39/100], Step [20/28], Loss: 0.4813\n",
      "Epoch [40/100], Step [10/28], Loss: 0.5430\n",
      "Epoch [40/100], Step [20/28], Loss: 0.5824\n",
      "Epoch [41/100], Step [10/28], Loss: 0.4829\n",
      "Epoch [41/100], Step [20/28], Loss: 0.5651\n",
      "Epoch [42/100], Step [10/28], Loss: 0.5217\n",
      "Epoch [42/100], Step [20/28], Loss: 0.4792\n",
      "Epoch [43/100], Step [10/28], Loss: 0.4693\n",
      "Epoch [43/100], Step [20/28], Loss: 0.5789\n",
      "Epoch [44/100], Step [10/28], Loss: 0.5483\n",
      "Epoch [44/100], Step [20/28], Loss: 0.5804\n",
      "Epoch [45/100], Step [10/28], Loss: 0.6277\n",
      "Epoch [45/100], Step [20/28], Loss: 0.4898\n",
      "Epoch [46/100], Step [10/28], Loss: 0.5524\n",
      "Epoch [46/100], Step [20/28], Loss: 0.5527\n",
      "Epoch [47/100], Step [10/28], Loss: 0.6190\n",
      "Epoch [47/100], Step [20/28], Loss: 0.6152\n",
      "Epoch [48/100], Step [10/28], Loss: 0.4592\n",
      "Epoch [48/100], Step [20/28], Loss: 0.5723\n",
      "Epoch [49/100], Step [10/28], Loss: 0.5792\n",
      "Epoch [49/100], Step [20/28], Loss: 0.5670\n",
      "Epoch [50/100], Step [10/28], Loss: 0.5256\n",
      "Epoch [50/100], Step [20/28], Loss: 0.4852\n",
      "Epoch [51/100], Step [10/28], Loss: 0.5195\n",
      "Epoch [51/100], Step [20/28], Loss: 0.5147\n",
      "Epoch [52/100], Step [10/28], Loss: 0.5796\n",
      "Epoch [52/100], Step [20/28], Loss: 0.5185\n",
      "Epoch [53/100], Step [10/28], Loss: 0.6259\n",
      "Epoch [53/100], Step [20/28], Loss: 0.4981\n",
      "Epoch [54/100], Step [10/28], Loss: 0.4882\n",
      "Epoch [54/100], Step [20/28], Loss: 0.4832\n",
      "Epoch [55/100], Step [10/28], Loss: 0.5015\n",
      "Epoch [55/100], Step [20/28], Loss: 0.4940\n",
      "Epoch [56/100], Step [10/28], Loss: 0.5587\n",
      "Epoch [56/100], Step [20/28], Loss: 0.4814\n",
      "Epoch [57/100], Step [10/28], Loss: 0.6074\n",
      "Epoch [57/100], Step [20/28], Loss: 0.5433\n",
      "Epoch [58/100], Step [10/28], Loss: 0.5058\n",
      "Epoch [58/100], Step [20/28], Loss: 0.5211\n",
      "Epoch [59/100], Step [10/28], Loss: 0.4712\n",
      "Epoch [59/100], Step [20/28], Loss: 0.5217\n",
      "Epoch [60/100], Step [10/28], Loss: 0.4474\n",
      "Epoch [60/100], Step [20/28], Loss: 0.5230\n",
      "Epoch [61/100], Step [10/28], Loss: 0.5192\n",
      "Epoch [61/100], Step [20/28], Loss: 0.5314\n",
      "Epoch [62/100], Step [10/28], Loss: 0.5450\n",
      "Epoch [62/100], Step [20/28], Loss: 0.5102\n",
      "Epoch [63/100], Step [10/28], Loss: 0.6080\n",
      "Epoch [63/100], Step [20/28], Loss: 0.5111\n",
      "Epoch [64/100], Step [10/28], Loss: 0.5196\n",
      "Epoch [64/100], Step [20/28], Loss: 0.5298\n",
      "Epoch [65/100], Step [10/28], Loss: 0.4209\n",
      "Epoch [65/100], Step [20/28], Loss: 0.4580\n",
      "Epoch [66/100], Step [10/28], Loss: 0.4372\n",
      "Epoch [66/100], Step [20/28], Loss: 0.4803\n",
      "Epoch [67/100], Step [10/28], Loss: 0.5283\n",
      "Epoch [67/100], Step [20/28], Loss: 0.5744\n",
      "Epoch [68/100], Step [10/28], Loss: 0.5209\n",
      "Epoch [68/100], Step [20/28], Loss: 0.4957\n",
      "Epoch [69/100], Step [10/28], Loss: 0.4993\n",
      "Epoch [69/100], Step [20/28], Loss: 0.5403\n",
      "Epoch [70/100], Step [10/28], Loss: 0.5377\n",
      "Epoch [70/100], Step [20/28], Loss: 0.5111\n",
      "Epoch [71/100], Step [10/28], Loss: 0.4410\n",
      "Epoch [71/100], Step [20/28], Loss: 0.5742\n",
      "Epoch [72/100], Step [10/28], Loss: 0.5557\n",
      "Epoch [72/100], Step [20/28], Loss: 0.5736\n",
      "Epoch [73/100], Step [10/28], Loss: 0.5431\n",
      "Epoch [73/100], Step [20/28], Loss: 0.5084\n",
      "Epoch [74/100], Step [10/28], Loss: 0.4539\n",
      "Epoch [74/100], Step [20/28], Loss: 0.5279\n",
      "Epoch [75/100], Step [10/28], Loss: 0.5625\n",
      "Epoch [75/100], Step [20/28], Loss: 0.5985\n",
      "Epoch [76/100], Step [10/28], Loss: 0.4818\n",
      "Epoch [76/100], Step [20/28], Loss: 0.5487\n",
      "Epoch [77/100], Step [10/28], Loss: 0.5173\n",
      "Epoch [77/100], Step [20/28], Loss: 0.4634\n",
      "Epoch [78/100], Step [10/28], Loss: 0.4144\n",
      "Epoch [78/100], Step [20/28], Loss: 0.5065\n",
      "Epoch [79/100], Step [10/28], Loss: 0.5763\n",
      "Epoch [79/100], Step [20/28], Loss: 0.5484\n",
      "Epoch [80/100], Step [10/28], Loss: 0.5072\n",
      "Epoch [80/100], Step [20/28], Loss: 0.5706\n",
      "Epoch [81/100], Step [10/28], Loss: 0.4941\n",
      "Epoch [81/100], Step [20/28], Loss: 0.5811\n",
      "Epoch [82/100], Step [10/28], Loss: 0.5441\n",
      "Epoch [82/100], Step [20/28], Loss: 0.5359\n",
      "Epoch [83/100], Step [10/28], Loss: 0.4769\n",
      "Epoch [83/100], Step [20/28], Loss: 0.4423\n",
      "Epoch [84/100], Step [10/28], Loss: 0.5226\n",
      "Epoch [84/100], Step [20/28], Loss: 0.4253\n",
      "Epoch [85/100], Step [10/28], Loss: 0.3993\n",
      "Epoch [85/100], Step [20/28], Loss: 0.5644\n",
      "Epoch [86/100], Step [10/28], Loss: 0.5655\n",
      "Epoch [86/100], Step [20/28], Loss: 0.4817\n",
      "Epoch [87/100], Step [10/28], Loss: 0.4660\n",
      "Epoch [87/100], Step [20/28], Loss: 0.4671\n",
      "Epoch [88/100], Step [10/28], Loss: 0.3909\n",
      "Epoch [88/100], Step [20/28], Loss: 0.5132\n",
      "Epoch [89/100], Step [10/28], Loss: 0.4581\n",
      "Epoch [89/100], Step [20/28], Loss: 0.4290\n",
      "Epoch [90/100], Step [10/28], Loss: 0.4593\n",
      "Epoch [90/100], Step [20/28], Loss: 0.5350\n",
      "Epoch [91/100], Step [10/28], Loss: 0.5114\n",
      "Epoch [91/100], Step [20/28], Loss: 0.5615\n",
      "Epoch [92/100], Step [10/28], Loss: 0.4872\n",
      "Epoch [92/100], Step [20/28], Loss: 0.4641\n",
      "Epoch [93/100], Step [10/28], Loss: 0.4956\n",
      "Epoch [93/100], Step [20/28], Loss: 0.4407\n",
      "Epoch [94/100], Step [10/28], Loss: 0.5153\n",
      "Epoch [94/100], Step [20/28], Loss: 0.4136\n",
      "Epoch [95/100], Step [10/28], Loss: 0.4906\n",
      "Epoch [95/100], Step [20/28], Loss: 0.4689\n",
      "Epoch [96/100], Step [10/28], Loss: 0.5100\n",
      "Epoch [96/100], Step [20/28], Loss: 0.4649\n",
      "Epoch [97/100], Step [10/28], Loss: 0.3950\n",
      "Epoch [97/100], Step [20/28], Loss: 0.4582\n",
      "Epoch [98/100], Step [10/28], Loss: 0.4447\n",
      "Epoch [98/100], Step [20/28], Loss: 0.5294\n",
      "Epoch [99/100], Step [10/28], Loss: 0.4945\n",
      "Epoch [99/100], Step [20/28], Loss: 0.4691\n",
      "Epoch [100/100], Step [10/28], Loss: 0.5003\n",
      "Epoch [100/100], Step [20/28], Loss: 0.5141\n"
     ]
    }
   ],
   "source": [
    "vanilla_model = TitanicModel()\n",
    "\n",
    "optimizer = torch.optim.Adam(vanilla_model.parameters(), lr=0.001)\n",
    "\n",
    "trained_vanilla_model = train(train_dataloader, vanilla_model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [10/28], Loss: 0.6628\n",
      "Epoch [1/100], Step [20/28], Loss: 0.7179\n",
      "Epoch [2/100], Step [10/28], Loss: 0.6627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/opacus/privacy_engine.py:760: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
      "  \"A ``sample_rate`` has been provided.\"\n",
      "/opt/conda/lib/python3.6/site-packages/opacus/privacy_engine.py:237: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py:796: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Step [20/28], Loss: 0.6784\n",
      "Epoch [3/100], Step [10/28], Loss: 0.6902\n",
      "Epoch [3/100], Step [20/28], Loss: 0.6628\n",
      "Epoch [4/100], Step [10/28], Loss: 0.7175\n",
      "Epoch [4/100], Step [20/28], Loss: 0.6909\n",
      "Epoch [5/100], Step [10/28], Loss: 0.6885\n",
      "Epoch [5/100], Step [20/28], Loss: 0.6244\n",
      "Epoch [6/100], Step [10/28], Loss: 0.5996\n",
      "Epoch [6/100], Step [20/28], Loss: 0.6499\n",
      "Epoch [7/100], Step [10/28], Loss: 0.6773\n",
      "Epoch [7/100], Step [20/28], Loss: 0.6247\n",
      "Epoch [8/100], Step [10/28], Loss: 0.6399\n",
      "Epoch [8/100], Step [20/28], Loss: 0.6263\n",
      "Epoch [9/100], Step [10/28], Loss: 0.6517\n",
      "Epoch [9/100], Step [20/28], Loss: 0.6881\n",
      "Epoch [10/100], Step [10/28], Loss: 0.6200\n",
      "Epoch [10/100], Step [20/28], Loss: 0.6658\n",
      "Epoch [11/100], Step [10/28], Loss: 0.6766\n",
      "Epoch [11/100], Step [20/28], Loss: 0.6553\n",
      "Epoch [12/100], Step [10/28], Loss: 0.7096\n",
      "Epoch [12/100], Step [20/28], Loss: 0.6566\n",
      "Epoch [13/100], Step [10/28], Loss: 0.6769\n",
      "Epoch [13/100], Step [20/28], Loss: 0.6554\n",
      "Epoch [14/100], Step [10/28], Loss: 0.6774\n",
      "Epoch [14/100], Step [20/28], Loss: 0.6767\n",
      "Epoch [15/100], Step [10/28], Loss: 0.6649\n",
      "Epoch [15/100], Step [20/28], Loss: 0.7604\n",
      "Epoch [16/100], Step [10/28], Loss: 0.6634\n",
      "Epoch [16/100], Step [20/28], Loss: 0.6635\n",
      "Epoch [17/100], Step [10/28], Loss: 0.7145\n",
      "Epoch [17/100], Step [20/28], Loss: 0.7266\n",
      "Epoch [18/100], Step [10/28], Loss: 0.6501\n",
      "Epoch [18/100], Step [20/28], Loss: 0.6352\n",
      "Epoch [19/100], Step [10/28], Loss: 0.6616\n",
      "Epoch [19/100], Step [20/28], Loss: 0.6902\n",
      "Epoch [20/100], Step [10/28], Loss: 0.6344\n",
      "Epoch [20/100], Step [20/28], Loss: 0.6895\n",
      "Epoch [21/100], Step [10/28], Loss: 0.6612\n",
      "Epoch [21/100], Step [20/28], Loss: 0.6759\n",
      "Epoch [22/100], Step [10/28], Loss: 0.6196\n",
      "Epoch [22/100], Step [20/28], Loss: 0.7041\n",
      "Epoch [23/100], Step [10/28], Loss: 0.6619\n",
      "Epoch [23/100], Step [20/28], Loss: 0.6627\n",
      "Epoch [24/100], Step [10/28], Loss: 0.6366\n",
      "Epoch [24/100], Step [20/28], Loss: 0.6890\n",
      "Epoch [25/100], Step [10/28], Loss: 0.7032\n",
      "Epoch [25/100], Step [20/28], Loss: 0.6628\n",
      "Epoch [26/100], Step [10/28], Loss: 0.6900\n",
      "Epoch [26/100], Step [20/28], Loss: 0.6194\n",
      "Epoch [27/100], Step [10/28], Loss: 0.6480\n",
      "Epoch [27/100], Step [20/28], Loss: 0.6336\n",
      "Epoch [28/100], Step [10/28], Loss: 0.6329\n",
      "Epoch [28/100], Step [20/28], Loss: 0.6189\n",
      "Epoch [29/100], Step [10/28], Loss: 0.6912\n",
      "Epoch [29/100], Step [20/28], Loss: 0.6914\n",
      "Epoch [30/100], Step [10/28], Loss: 0.7059\n",
      "Epoch [30/100], Step [20/28], Loss: 0.6183\n",
      "Epoch [31/100], Step [10/28], Loss: 0.6470\n",
      "Epoch [31/100], Step [20/28], Loss: 0.7081\n",
      "Epoch [32/100], Step [10/28], Loss: 0.6163\n",
      "Epoch [32/100], Step [20/28], Loss: 0.6780\n",
      "Epoch [33/100], Step [10/28], Loss: 0.6768\n",
      "Epoch [33/100], Step [20/28], Loss: 0.6345\n",
      "Epoch [34/100], Step [10/28], Loss: 0.6632\n",
      "Epoch [34/100], Step [20/28], Loss: 0.6484\n",
      "Epoch [35/100], Step [10/28], Loss: 0.6914\n",
      "Epoch [35/100], Step [20/28], Loss: 0.7186\n",
      "Epoch [36/100], Step [10/28], Loss: 0.6635\n",
      "Epoch [36/100], Step [20/28], Loss: 0.7051\n",
      "Epoch [37/100], Step [10/28], Loss: 0.6071\n",
      "Epoch [37/100], Step [20/28], Loss: 0.6629\n",
      "Epoch [38/100], Step [10/28], Loss: 0.5803\n",
      "Epoch [38/100], Step [20/28], Loss: 0.6775\n",
      "Epoch [39/100], Step [10/28], Loss: 0.6631\n",
      "Epoch [39/100], Step [20/28], Loss: 0.7049\n",
      "Epoch [40/100], Step [10/28], Loss: 0.5474\n",
      "Epoch [40/100], Step [20/28], Loss: 0.7058\n",
      "Epoch [41/100], Step [10/28], Loss: 0.6918\n",
      "Epoch [41/100], Step [20/28], Loss: 0.6631\n",
      "Epoch [42/100], Step [10/28], Loss: 0.6770\n",
      "Epoch [42/100], Step [20/28], Loss: 0.6785\n",
      "Epoch [43/100], Step [10/28], Loss: 0.6942\n",
      "Epoch [43/100], Step [20/28], Loss: 0.7111\n",
      "Epoch [44/100], Step [10/28], Loss: 0.6142\n",
      "Epoch [44/100], Step [20/28], Loss: 0.6289\n",
      "Epoch [45/100], Step [10/28], Loss: 0.7312\n",
      "Epoch [45/100], Step [20/28], Loss: 0.6791\n",
      "Epoch [46/100], Step [10/28], Loss: 0.6283\n",
      "Epoch [46/100], Step [20/28], Loss: 0.6305\n",
      "Epoch [47/100], Step [10/28], Loss: 0.6796\n",
      "Epoch [47/100], Step [20/28], Loss: 0.6626\n",
      "Epoch [48/100], Step [10/28], Loss: 0.6141\n",
      "Epoch [48/100], Step [20/28], Loss: 0.6803\n",
      "Epoch [49/100], Step [10/28], Loss: 0.6459\n",
      "Epoch [49/100], Step [20/28], Loss: 0.6141\n",
      "Epoch [50/100], Step [10/28], Loss: 0.6289\n",
      "Epoch [50/100], Step [20/28], Loss: 0.7820\n",
      "Epoch [51/100], Step [10/28], Loss: 0.6800\n",
      "Epoch [51/100], Step [20/28], Loss: 0.6628\n",
      "Epoch [52/100], Step [10/28], Loss: 0.6465\n",
      "Epoch [52/100], Step [20/28], Loss: 0.6637\n",
      "Epoch [53/100], Step [10/28], Loss: 0.6287\n",
      "Epoch [53/100], Step [20/28], Loss: 0.7337\n",
      "Epoch [54/100], Step [10/28], Loss: 0.5744\n",
      "Epoch [54/100], Step [20/28], Loss: 0.6624\n",
      "Epoch [55/100], Step [10/28], Loss: 0.7350\n",
      "Epoch [55/100], Step [20/28], Loss: 0.5706\n",
      "Epoch [56/100], Step [10/28], Loss: 0.6444\n",
      "Epoch [56/100], Step [20/28], Loss: 0.6809\n",
      "Epoch [57/100], Step [10/28], Loss: 0.7755\n",
      "Epoch [57/100], Step [20/28], Loss: 0.6082\n",
      "Epoch [58/100], Step [10/28], Loss: 0.6452\n",
      "Epoch [58/100], Step [20/28], Loss: 0.6641\n",
      "Epoch [59/100], Step [10/28], Loss: 0.6839\n",
      "Epoch [59/100], Step [20/28], Loss: 0.6249\n",
      "Epoch [60/100], Step [10/28], Loss: 0.6826\n",
      "Epoch [60/100], Step [20/28], Loss: 0.5681\n",
      "Epoch [61/100], Step [10/28], Loss: 0.6440\n",
      "Epoch [61/100], Step [20/28], Loss: 0.6060\n",
      "Epoch [62/100], Step [10/28], Loss: 0.6840\n",
      "Epoch [62/100], Step [20/28], Loss: 0.6651\n",
      "Epoch [63/100], Step [10/28], Loss: 0.7061\n",
      "Epoch [63/100], Step [20/28], Loss: 0.6057\n",
      "Epoch [64/100], Step [10/28], Loss: 0.7643\n",
      "Epoch [64/100], Step [20/28], Loss: 0.7038\n",
      "Epoch [65/100], Step [10/28], Loss: 0.7028\n",
      "Epoch [65/100], Step [20/28], Loss: 0.7424\n",
      "Epoch [66/100], Step [10/28], Loss: 0.6253\n",
      "Epoch [66/100], Step [20/28], Loss: 0.6054\n",
      "Epoch [67/100], Step [10/28], Loss: 0.7033\n",
      "Epoch [67/100], Step [20/28], Loss: 0.7022\n",
      "Epoch [68/100], Step [10/28], Loss: 0.7421\n",
      "Epoch [68/100], Step [20/28], Loss: 0.7221\n",
      "Epoch [69/100], Step [10/28], Loss: 0.7414\n",
      "Epoch [69/100], Step [20/28], Loss: 0.7016\n",
      "Epoch [70/100], Step [10/28], Loss: 0.6819\n",
      "Epoch [70/100], Step [20/28], Loss: 0.7204\n",
      "Epoch [71/100], Step [10/28], Loss: 0.7002\n",
      "Epoch [71/100], Step [20/28], Loss: 0.5861\n",
      "Epoch [72/100], Step [10/28], Loss: 0.7203\n",
      "Epoch [72/100], Step [20/28], Loss: 0.7778\n",
      "Epoch [73/100], Step [10/28], Loss: 0.7019\n",
      "Epoch [73/100], Step [20/28], Loss: 0.6820\n",
      "Epoch [74/100], Step [10/28], Loss: 0.7014\n",
      "Epoch [74/100], Step [20/28], Loss: 0.6621\n",
      "Epoch [75/100], Step [10/28], Loss: 0.6059\n",
      "Epoch [75/100], Step [20/28], Loss: 0.5865\n",
      "Epoch [76/100], Step [10/28], Loss: 0.6632\n",
      "Epoch [76/100], Step [20/28], Loss: 0.6623\n",
      "Epoch [77/100], Step [10/28], Loss: 0.6815\n",
      "Epoch [77/100], Step [20/28], Loss: 0.5879\n",
      "Epoch [78/100], Step [10/28], Loss: 0.5683\n",
      "Epoch [78/100], Step [20/28], Loss: 0.7178\n",
      "Epoch [79/100], Step [10/28], Loss: 0.6797\n",
      "Epoch [79/100], Step [20/28], Loss: 0.7485\n",
      "Epoch [80/100], Step [10/28], Loss: 0.6273\n",
      "Epoch [80/100], Step [20/28], Loss: 0.7623\n",
      "Epoch [81/100], Step [10/28], Loss: 0.7292\n",
      "Epoch [81/100], Step [20/28], Loss: 0.7118\n",
      "Epoch [82/100], Step [10/28], Loss: 0.7297\n",
      "Epoch [82/100], Step [20/28], Loss: 0.6958\n",
      "Epoch [83/100], Step [10/28], Loss: 0.6432\n",
      "Epoch [83/100], Step [20/28], Loss: 0.6789\n",
      "Epoch [84/100], Step [10/28], Loss: 0.7294\n",
      "Epoch [84/100], Step [20/28], Loss: 0.6784\n",
      "Epoch [85/100], Step [10/28], Loss: 0.6779\n",
      "Epoch [85/100], Step [20/28], Loss: 0.6775\n",
      "Epoch [86/100], Step [10/28], Loss: 0.6613\n",
      "Epoch [86/100], Step [20/28], Loss: 0.7273\n",
      "Epoch [87/100], Step [10/28], Loss: 0.6124\n",
      "Epoch [87/100], Step [20/28], Loss: 0.6942\n",
      "Epoch [88/100], Step [10/28], Loss: 0.6452\n",
      "Epoch [88/100], Step [20/28], Loss: 0.7105\n",
      "Epoch [89/100], Step [10/28], Loss: 0.5630\n",
      "Epoch [89/100], Step [20/28], Loss: 0.6450\n",
      "Epoch [90/100], Step [10/28], Loss: 0.6929\n",
      "Epoch [90/100], Step [20/28], Loss: 0.6293\n",
      "Epoch [91/100], Step [10/28], Loss: 0.6297\n",
      "Epoch [91/100], Step [20/28], Loss: 0.6921\n",
      "Epoch [92/100], Step [10/28], Loss: 0.6145\n",
      "Epoch [92/100], Step [20/28], Loss: 0.6608\n",
      "Epoch [93/100], Step [10/28], Loss: 0.5984\n",
      "Epoch [93/100], Step [20/28], Loss: 0.6453\n",
      "Epoch [94/100], Step [10/28], Loss: 0.6455\n",
      "Epoch [94/100], Step [20/28], Loss: 0.6928\n",
      "Epoch [95/100], Step [10/28], Loss: 0.6296\n",
      "Epoch [95/100], Step [20/28], Loss: 0.6921\n",
      "Epoch [96/100], Step [10/28], Loss: 0.6304\n",
      "Epoch [96/100], Step [20/28], Loss: 0.6286\n",
      "Epoch [97/100], Step [10/28], Loss: 0.6145\n",
      "Epoch [97/100], Step [20/28], Loss: 0.6300\n",
      "Epoch [98/100], Step [10/28], Loss: 0.6896\n",
      "Epoch [98/100], Step [20/28], Loss: 0.6605\n",
      "Epoch [99/100], Step [10/28], Loss: 0.6595\n",
      "Epoch [99/100], Step [20/28], Loss: 0.6454\n",
      "Epoch [100/100], Step [10/28], Loss: 0.6444\n",
      "Epoch [100/100], Step [20/28], Loss: 0.6301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:apscheduler.executors.default:Running job \"BaseEmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2022-03-10 17:01:17 UTC)\" (scheduled at 2022-03-10 17:01:17.702083+00:00)\n",
      "INFO:apscheduler.executors.default:Job \"BaseEmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2022-03-10 17:01:32 UTC)\" executed successfully\n"
     ]
    }
   ],
   "source": [
    "# Calculate Differential Privacy\n",
    "noise_multiplier = 5\n",
    "max_per_sample_grad_norm = 1.5\n",
    "sample_rate = batch_size/len(train_dataset)\n",
    "\n",
    "dp_model = TitanicModel()\n",
    "\n",
    "optimizer = optim.Adam(dp_model.parameters(), weight_decay=0.0001, lr=0.003)\n",
    "\n",
    "privacy_engine = PrivacyEngine(\n",
    "    dp_model,\n",
    "    max_grad_norm=max_per_sample_grad_norm,\n",
    "    noise_multiplier = noise_multiplier,\n",
    "    sample_rate = sample_rate,\n",
    ")\n",
    "\n",
    "privacy_engine.attach(optimizer)\n",
    "trained_dp_model = train(train_dataloader, dp_model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ε = 1.83, δ = 1e-06\n"
     ]
    }
   ],
   "source": [
    "epsilon, best_alpha = privacy_engine.get_privacy_spent()\n",
    "print (f\" ε = {epsilon:.2f}, δ = {privacy_engine.target_delta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:apscheduler.scheduler:Scheduler has been shut down\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000219429418842438\n"
     ]
    }
   ],
   "source": [
    "emissions: float = tracker.stop()\n",
    "print(emissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explainability\n",
    "from captum.attr import IntegratedGradients\n",
    "ig = IntegratedGradients(trained_vanilla_model)\n",
    "\n",
    "feature_tensor.requires_grad_()\n",
    "attr, delta = ig.attribute(feature_tensor,target=1, return_convergence_delta=True)\n",
    "attr = attr.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Feature Importances\n",
      "Pclass :  -0.276\n",
      "Age :  -0.176\n",
      "SibSp :  -0.087\n",
      "Parch :  -0.035\n",
      "Fare :  0.579\n",
      "Sex_female :  0.290\n",
      "Sex_male :  -0.188\n",
      "Embarked_C :  0.024\n",
      "Embarked_Q :  0.013\n",
      "Embarked_S :  -0.066\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAssAAAGECAYAAADA7m3/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAlVElEQVR4nO3deZhlVX3u8e8LDYiCONAqMjUieoOIrXRwiFFUHAgGiCJDvCJGBb3RGK9G8WoQpwTneZ5nRZRrX0VxQNQ4II00ICAy2AiK2CoYcQDB3/1jr5JDUaunOlXVVH8/z3Oe3mdPa61du6rfvfba56SqkCRJknRTG811BSRJkqT1lWFZkiRJ6jAsS5IkSR2GZUmSJKnDsCxJkiR1GJYlSZKkDsOyJEmS1GFYljQnkpyS5Mokm811XaYryTFJ/pTk6pHX88awz4+Mq45rUN6iJJVkwWyVuSqtLned63pIkmFZ0qxLsgj4W6CA/WZg/3MR+D5ZVVuMvF41B3X4i/Ul9K6tm2u9Jc1fhmVJc+Ew4LvAB4AnAiTZLMlVSXabWCnJwiR/SHKH9v7RSZa39b6dZPeRdVckeX6Ss4DfJVmQ5KgkFyX5bZJzk/zDyPobJ3ltkl8m+XGSZ4z2rCbZKsl7k1ye5KdJXp5k47VtaJJ/SnJe60U/KcmOI8vemOTSJP+d5PQkf9vmPwr4P8DBrZf6zJE27j2y/V96n0d6hp+c5CfAyasrfzX1/kCStyX5QqvDt5LcKckb2r5+mOTek47/C9pxvjLJ+5PcYmT5U5NcmOTXSZYmufPIskryz0kuAC5I8o226MxW9sFJbpvkc0lWtv1/Lsl2I/s4JcnLWj1/m+RLSbYeWf7Ads5c1Y754W3+Zklek+QnSa5I8o4km7dlW7dyrmr1/mYS/9+UNjD+0kuaC4cBH22vRya5Y1VdA3wGOHRkvYOAr1fVL1owex9wJHB74J3A0tx4GMehwL7AbarqOuAihh7srYCXAB9Jsk1b96nAPsBi4D7AAZPq+AHgOuCuwL2BRwBPWZtGJtmfIfQ+BlgIfBP4+Mgqp7Xybwd8DPhUkltU1ReB/+CG3up7rUWxDwb+iuG4rq781TkIeBGwNXAN8B3g++398cDrJq3/eOCRwM7A3dq2JHko8J9tf9sAlwCfmLTtAcB9gV2r6kFt3r1a+z/J8P/V+4EdgR2APwBvmbSPfwSeBNwB2BR4bit/R+ALwJvbcVgMLG/bHNvqupjhZ70tcHRb9hzgsrbNHRmOZfUOlqR5qqp8+fLla9ZewAOBPwFbt/c/BJ7dpvcGLhpZ91vAYW367cDLJu3rfODBbXoF8E+rKXs5sH+bPhk4cmTZ3gxBaAFDMLoG2Hxk+aHA1zr7PQa4Frhq5HVnhoD25JH1NgJ+D+zY2c+VDAFxYp8fmbR8BbD3pHI/0qYXtfrfZWT5Gpc/sv2C9v4DwLtHlj8TOG/k/T2BqybV7Wkj7/9u4mcJvBd41ciyLdo5sKi9L+Chk+pTwF1X8bNcDFw58v4U4EUj7/8X8MU2/QLghCn2EeB3wM4j8+4P/LhNvxT47Krq4cuXr/n/smdZ0mx7IvClqvple/+xNg/ga8Atk9w3w7jmxcAJbdmOwHPaLfGrklwFbM8QSidcOlpQksNGhm1cBezG0CtK2+7SzrY7ApsAl49s+06GHsue46rqNiOvn7X9vHFkH79mCGjbtvo9tw2R+E1bvtVI/dbV5HZ0y18DV4xM/2GK91usouxLuOFnc+f2HoCquhr41aR63OhnN1mSWyZ5Z5JLkvw38A3gNpOGxvx8ZPr3I/XbnuEuw2QLgVsCp48coy+2+QCvBi4EvpTk4iRHraqOkuYnH6SQNGvaWNCDgI2TTASbzRhCz72q6swkxzH04l4BfK6qftvWuxR4RVW9YhVF/OUWebv1/m7gYcB3qur6JMsZwiLA5cB2I9tuPzJ9KUPP8tY1DOdYVxN1/ujkBW188vNa/c6pqj8nuXKkflPd7v8dQ7ibcKcp1hndrlv+DBk9hjsAP2vTExcOACS5FcNQmp+OrL+64Q3PAe4O3Leqfp5kMXAGNxyvVbkU2HOK+b9kCP33qKqfTl7Yzr3nMFyk7QacnOS0qvrqGpQpaZ6wZ1nSbDoAuB7YlaHXeDHD+NpvMoxjhqGn+WCG8a8fG9n23cDTWq9zktwqyb5JtuyUdSuGALYSIMmTGHqWJxwHPCvJtkluAzx/YkFVXQ58CXhtklsn2SjJzkkevJbtfQfwgiT3aHXYKsnj2rItGcZErwQWJDkauPXItlcAiyY9ULYcOCTJJkmWAAdOo/yZ8M9JtktyO+CFwCfb/I8DT0qyuI0x/w/g1KpasYp9XQHcZeT9lgzB9qq2/xevRb0+Cuyd5KAMD37ePsniqvozw3n1+tzwEOm2SR7Zph+d5K5JAvyG4dz981qUK2keMCxLmk1PBN5fVT+pqp9PvBge1Hp8kgVVdSpDD+rEmF8AqmoZw0N5b2EY23shcHivoKo6F3gtw0NpVzCMsf3WyCrvZgjEZzH0UJ7IEF6vb8sPY3hI7NxW3vEMD6etsao6AXgl8Ik2dOAHDA8VApzEcMv/RwxDFP7IjYcifKr9+6sk32/T/87w8NyVDA8sjl5MrG35M+FjDMf0YoZhDy9v9fgKQ90/zdCjvzNwyGr2dQzwwTY84iDgDcDmDL3B32U4dmukqn7CMIb6OQxDUZYDEw9NPp/hXPpuO0ZfYejBBtilvb+a4Tx6W1V9bU3LlTQ/pMoHeyUpyT7AO6pqjT5aTTeWZAXwlBaMJWnesGdZ0gYpyeZJ/q7dlt+W4bb+CavbTpK0YTEsS9pQhWEow5UMwzDO44bP15UkCXAYhiRJktRlz7IkSZLUYViWJEmSOtbbLyXZeuuta9GiRXNdDUmSJM1zp59++i+rauFUy9bbsLxo0SKWLVs219WQJEnSPJfkkt4yh2FIkiRJHYZlSZIkqWMsYTnJo5Kcn+TCJEd11jkoyblJzkmyyq9olSRJktYH0x6znGRj4K3Aw4HLgNOSLK2qc0fW2QV4AfA3VXVlkjtMt1xJkiRppo2jZ3lP4MKquriqrgU+Aew/aZ2nAm+tqisBquoXYyhXkiRJmlHjCMvbApeOvL+szRt1N+BuSb6V5LtJHjXVjpIckWRZkmUrV64cQ9UkSZKkdTdbD/gtAHYB9gIOBd6d5DaTV6qqd1XVkqpasnDhlB91J0mSJM2acYTlnwLbj7zfrs0bdRmwtKr+VFU/Bn7EEJ4lSZKk9dY4wvJpwC5JdkqyKXAIsHTSOv+XoVeZJFszDMu4eAxlS5IkSTNm2mG5qq4DngGcBJwHHFdV5yR5aZL92monAb9Kci7wNeDfqupX0y1bkiRJmkmpqrmuw5SWLFlSft21JEmSZlqS06tqyVTL/AY/SZIkqcOwLEmSJHVM+xv8JOnmZNFRn5/rKozFimP3nesqSNIGwZ5lSZIkqcOwLEmSJHUYliVJkqQOw7IkSZLUYViWJEmSOgzLkiRJUodhWZIkSeowLEuSJEkdhmVJkiSpw7AsSZIkdRiWJUmSpA7DsiRJktRhWJYkSZI6DMuSJElSh2FZkiRJ6jAsS5IkSR2GZUmSJKnDsCxJkiR1GJYlSZKkDsOyJEmS1GFYliRJkjoMy5IkSVKHYVmSJEnqMCxLkiRJHYZlSZIkqcOwLEmSJHUYliVJkqQOw7IkSZLUYViWJEmSOgzLkiRJUodhWZIkSeowLEuSJEkdhmVJkiSpw7AsSZIkdRiWJUmSpA7DsiRJktRhWJYkSZI6DMuSJElSh2FZkiRJ6jAsS5IkSR2GZUmSJKnDsCxJkiR1GJYlSZKkDsOyJEmS1GFYliRJkjoMy5IkSVKHYVmSJEnqMCxLkiRJHYZlSZIkqcOwLEmSJHUYliVJkqQOw7IkSZLUYViWJEmSOgzLkiRJUodhWZIkSeowLEuSJEkdhmVJkiSpYyxhOcmjkpyf5MIkR61ivccmqSRLxlGuJEmSNJOmHZaTbAy8FdgH2BU4NMmuU6y3JfAs4NTplilJkiTNhnH0LO8JXFhVF1fVtcAngP2nWO9lwCuBP46hTEmSJGnGjSMsbwtcOvL+sjbvL5LcB9i+qj4/hvIkSZKkWTHjD/gl2Qh4HfCcNVj3iCTLkixbuXLlTFdNkiRJWqVxhOWfAtuPvN+uzZuwJbAbcEqSFcD9gKVTPeRXVe+qqiVVtWThwoVjqJokSZK07sYRlk8DdkmyU5JNgUOApRMLq+o3VbV1VS2qqkXAd4H9qmrZGMqWJEmSZsy0w3JVXQc8AzgJOA84rqrOSfLSJPtNd/+SJEnSXFkwjp1U1YnAiZPmHd1Zd69xlClJkiTNNL/BT5IkSeowLEuSJEkdhmVJkiSpw7AsSZIkdRiWJUmSpA7DsiRJktRhWJYkSZI6DMuSJElSh2FZkiRJ6jAsS5IkSR2GZUmSJKnDsCxJkiR1GJYlSZKkDsOyJEmS1GFYliRJkjoMy5IkSVKHYVmSJEnqMCxLkiRJHYZlSZIkqcOwLEmSJHUYliVJkqQOw7IkSZLUYViWJEmSOgzLkiRJUodhWZIkSeowLEuSJEkdhmVJkiSpw7AsSZIkdRiWJUmSpA7DsiRJktRhWJYkSZI6DMuSJElSh2FZkiRJ6lgw1xWQJM28RUd9fq6rMBYrjt13rqsgaQNjz7IkSZLUYViWJEmSOgzLkiRJUodhWZIkSeowLEuSJEkdhmVJkiSpw7AsSZIkdRiWJUmSpA7DsiRJktRhWJYkSZI6DMuSJElSh2FZkiRJ6jAsS5IkSR2GZUmSJKnDsCxJkiR1GJYlSZKkDsOyJEmS1GFYliRJkjoMy5IkSVKHYVmSJEnqMCxLkiRJHYZlSZIkqcOwLEmSJHUYliVJkqQOw7IkSZLUYViWJEmSOgzLkiRJUsdYwnKSRyU5P8mFSY6aYvn/TnJukrOSfDXJjuMoV5IkSZpJ0w7LSTYG3grsA+wKHJpk10mrnQEsqardgeOBV023XEmSJGmmjaNneU/gwqq6uKquBT4B7D+6QlV9rap+395+F9huDOVKkiRJM2ocYXlb4NKR95e1eT1PBr4w1YIkRyRZlmTZypUrx1A1SZIkad3N6gN+Sf4nsAR49VTLq+pdVbWkqpYsXLhwNqsmSZIk3cSCMezjp8D2I++3a/NuJMnewAuBB1fVNWMoV5IkSZpR4+hZPg3YJclOSTYFDgGWjq6Q5N7AO4H9quoXYyhTkiRJmnHTDstVdR3wDOAk4DzguKo6J8lLk+zXVns1sAXwqSTLkyzt7E6SJElab4xjGAZVdSJw4qR5R49M7z2OciRJkqTZ5Df4SZIkSR2GZUmSJKnDsCxJkiR1GJYlSZKkDsOyJEmS1GFYliRJkjoMy5IkSVKHYVmSJEnqMCxLkiRJHYZlSZIkqcOwLEmSJHUYliVJkqQOw7IkSZLUYViWJEmSOgzLkiRJUodhWZIkSeowLEuSJEkdhmVJkiSpw7AsSZIkdRiWJUmSpA7DsiRJktRhWJYkSZI6DMuSJElSh2FZkiRJ6jAsS5IkSR2GZUmSJKnDsCxJkiR1GJYlSZKkDsOyJEmS1GFYliRJkjoMy5IkSVKHYVmSJEnqMCxLkiRJHYZlSZIkqcOwLEmSJHUYliVJkqQOw7IkSZLUYViWJEmSOgzLkiRJUodhWZIkSeowLEuSJEkdhmVJkiSpw7AsSZIkdRiWJUmSpA7DsiRJktRhWJYkSZI6DMuSJElSh2FZkiRJ6jAsS5IkSR2GZUmSJKnDsCxJkiR1GJYlSZKkDsOyJEmS1GFYliRJkjoMy5IkSVLHgrmugCRJGr9FR31+rqswbSuO3XeuqyDZsyxJkiT1GJYlSZKkDsOyJEmS1OGYZUmSNG84VlvjNpae5SSPSnJ+kguTHDXF8s2SfLItPzXJonGUK0mSJM2kaYflJBsDbwX2AXYFDk2y66TVngxcWVV3BV4PvHK65UqSJEkzbRw9y3sCF1bVxVV1LfAJYP9J6+wPfLBNHw88LEnGULYkSZI0Y1JV09tBciDwqKp6Snv/BOC+VfWMkXV+0Na5rL2/qK3zy0n7OgI4AmCHHXbY45JLLplW3dbVfBjvBGs/5sl237zZbmlq8+Fc9zzX6syH8xzm7lxPcnpVLZlq2Xr1aRhV9a6qWlJVSxYuXDjX1ZEkSdIGbhxh+afA9iPvt2vzplwnyQJgK+BXYyhbkiRJmjHjCMunAbsk2SnJpsAhwNJJ6ywFntimDwROrumO/5AkSZJm2LQ/Z7mqrkvyDOAkYGPgfVV1TpKXAsuqainwXuDDSS4Efs0QqCVJkqT12li+lKSqTgROnDTv6JHpPwKPG0dZkiRJ0mxZrx7wkyRJktYnhmVJkiSpw7AsSZIkdRiWJUmSpA7DsiRJktRhWJYkSZI6DMuSJElSx1g+Z1nSzc+KY/ed6ypIkrTes2dZkiRJ6jAsS5IkSR2GZUmSJKnDsCxJkiR1GJYlSZKkDsOyJEmS1GFYliRJkjoMy5IkSVKHYVmSJEnq8Bv8JEmSbub8VtaZY8+yJEmS1GFYliRJkjoMy5IkSVKHYVmSJEnqMCxLkiRJHYZlSZIkqcOwLEmSJHX4Ocva4PnZlJIkqceeZUmSJKnDsCxJkiR1GJYlSZKkDsOyJEmS1GFYliRJkjoMy5IkSVKHYVmSJEnqMCxLkiRJHYZlSZIkqcOwLEmSJHUYliVJkqQOw7IkSZLUYViWJEmSOgzLkiRJUodhWZIkSeowLEuSJEkdhmVJkiSpw7AsSZIkdRiWJUmSpA7DsiRJktRhWJYkSZI6DMuSJElSh2FZkiRJ6jAsS5IkSR2GZUmSJKnDsCxJkiR1GJYlSZKkDsOyJEmS1GFYliRJkjoWzHUFtP5Ycey+c10FSZKk9Yo9y5IkSVKHYVmSJEnqMCxLkiRJHY5ZliTNaz6PIWk6ptWznOR2Sb6c5IL2722nWGdxku8kOSfJWUkOnk6ZkiRJ0myZ7jCMo4CvVtUuwFfb+8l+DxxWVfcAHgW8IcltplmuJEmSNOOmG5b3Bz7Ypj8IHDB5har6UVVd0KZ/BvwCWDjNciVJkqQZN92wfMequrxN/xy446pWTrInsClw0TTLlSRJkmbcah/wS/IV4E5TLHrh6JuqqiS1iv1sA3wYeGJV/bmzzhHAEQA77LDD6qomSZIkzajVhuWq2ru3LMkVSbapqstbGP5FZ71bA58HXlhV311FWe8C3gWwZMmSbvCWJEmSZsN0h2EsBZ7Ypp8IfHbyCkk2BU4APlRVx0+zPEmSJGnWTDcsHws8PMkFwN7tPUmWJHlPW+cg4EHA4UmWt9fiaZYrSZIkzbhpfSlJVf0KeNgU85cBT2nTHwE+Mp1yJEmSpLng111LkiRJHYZlSZIkqcOwLEmSJHUYliVJkqQOw7IkSZLUYViWJEmSOgzLkiRJUodhWZIkSeowLEuSJEkdhmVJkiSpw7AsSZIkdRiWJUmSpA7DsiRJktRhWJYkSZI6DMuSJElSh2FZkiRJ6jAsS5IkSR2GZUmSJKnDsCxJkiR1GJYlSZKkDsOyJEmS1LFgriuwPlpx7L5zXQVJkiStB+xZliRJkjoMy5IkSVKHYVmSJEnqMCxLkiRJHYZlSZIkqcOwLEmSJHUYliVJkqQOw7IkSZLUYViWJEmSOgzLkiRJUodhWZIkSeowLEuSJEkdhmVJkiSpw7AsSZIkdaSq5roOU0qyErhkrusxg7YGfjnXlZgDtnvDYrs3LLZ7w7Ohtt12zz87VtXCqRast2F5vkuyrKqWzHU9Zpvt3rDY7g2L7d7wbKhtt90bFodhSJIkSR2GZUmSJKnDsDx33jXXFZgjtnvDYrs3LLZ7w7Ohtt12b0AcsyxJkiR12LMsSZIkdRiWpyHJ9UmWJ/lBkk8lueUq1j0myXNns35zIckBSSrJ/5jrusykJC9Mck6Ss9o5cN8k70mya1t+dWe7+yU5tW1zXpJjZrXi07A25/sa7m9Rkh+Mq36zZeQ4TLwWzXWdJEkzx7A8PX+oqsVVtRtwLfC0ua7QeuBQ4L/av/NSkvsDjwbuU1W7A3sDl1bVU6rq3NVs/kHgiKpaDOwGHDejlR2vdTrfkyyY2WrNuonjMPFasboNMpiTv7dTXdiNab+Paxd8XxvH/jplHJ7kLWPc34wci2nWadoXjVNcwB21FtvuleRz0yz/lCTr9HFiST6Q5MBVLN8kybFJLkjy/STfSbJPWzaf271pkjckubC9Ppdkh5Hl87ntj05yRpIzk5yb5Mh1r+l4zLf/xObSN4HdAZIcBjwXKOCsqnrC6IpJngocAWwKXAg8oap+n+RxwIuB64HfVNWDktwDeH9bdyPgsVV1wSy1aa0k2QJ4IPAQ4P8BL24B4S3AQ4FLgT8B76uq45PsAbwO2ILhQ84Pr6rL56Tya2cb4JdVdQ1AVf0Shj8ewHOrall7/3rgEcDPgUOqaiVwB+Dytt31wLlt3WOAnYG7Mnzo+6uq6t2z16S19k1g9yR/D7yI4fz8FfD4qrpipD13AX6S5F+Bd7T3AE8HfgZsnOTdwAOAnwL7V9UfZrMh09XO+88CtwU2AV5UVZ9tPc4nAacCewB/l+Qg4CBgM+CEqnrxDNdt9MLumiRbM/ysxuHJwFOr6r/GtL8ZNcPHYq79oV2Az7okG89wES9j+Ju7W/u53RF4cFs2n9v9H8CWwN2r6vokTwI+m2SPqvoz87TtSTZheIhwz6q6LMlmwKKZKm9N2bM8Bq3nbB/g7BZuXwQ8tKruBTxrik0+U1V/3Zafx/CfDsDRwCPb/P3avKcBb2y/FEuAy2auJdO2P/DFqvoR8KsWhh/DcKLvCjwBuD/85RfizcCBVbUH8D7gFXNR6XXwJWD7JD9K8rYkD55inVsBy6rqHsDXGS6CAF4PnJ/khCRHJrnFyDa7M1xU3B84OsmdZ7AN62z0fGe4i3C/qro38AngeSOr7grsXVWHAm8Cvt7O7fsA57R1dgHe2o7TVcBjZ6UR07P5SG/OCcAfgX+oqvswXCi+NknaursAb2vtu3t7vyewGNgjyYNmuK43ubCrqp8l2SPJ15OcnuSkJNsk2SrJ+UnuDpDk4+3C/iaSHM1wYfzeJK9OsnH797TWa3tkW2+vVs5nk1zceggfn+R7Sc5OsnNb7+8zDE86I8lXWiCaXObCJJ9uZZyW5G/Wh2PRll/d2n9Oq/+ereft4iT7tXUWJflmht7R7yd5wBT7mfI4rqskK5L8ZztXlyW5T2vjRUlG7wzdOsnnW5vf0To5SPL2tt05SV4yab+vTPJ94HEj8zfK0Gv48lWcE0nyllbWVxg6EHr1vyXwVOCZIz+3K6pqlXfk5km7nwQ8u3WqUFXvB65muJM5b9vOcIGwgKHzhaq6pqrOX1WbZ0VV+VrHF0MP8PL2ejNDL8UzgVdMse4xDL2OMFwVf5MhbPwYeEeb/w7gywx/HG7f5v0jQ7B4PrDLXLd5Ncfjc8DD2/S/AK8B3gA8aWSdzwAHMgxB+O+R43c28KW5bsNatHVjYC/gJQw9x4cDpwBLRs6NBW36LsDykW13ZuhZ/Tpwysj58dKRdT4EHDDX7ZzU5qnO93syXDycDZzPcLE00Z4Xj2y7Eths0v4WAReMvH8+Q6/snLd1Ncfh6knvN2G4e3JWOzZ/AO7U2vfjkfVeA6wYOYYXAk+e4bpu0cr6EfC29rdnE+DbwMK2zsEMd3sAHg58Bzhk4me5in2Pnu9HTPzsGHrNlwE7td+RqxiC6mYMdw9e0tZ7FvCGNn1bbvh0pqcAr23ThwNvadMfAx7YpncAzluPjkUB+7TpE9rvxCbAvWi/+8AtgVu06V0YLqYnfg9+sKrjuAZtG/3dXA4c3OavAJ7epl/PcI5uCSwErmjz92K44LsLw9+1LzN0YgDcrv27cft57z6y3+dNOhfuB3wceOFqzonHtDI2Bu7czo8DO+3aHTjDdv9l/uuBf53PbW/bvgf4Rdv344GNxvU3cV1fDsOYnpvcBslfOpRW6QMMQejMJIcznLhU1dMyjKHbFzg9w+2WjyU5tc07McmRVXXy+JowHklux9Ares8kxfBLUQz/cUy5CXBOVd1/lqo4VjVc7Z8CnJLkbOCJq9tkZNuLgLdnGH6wMsntJ6/TeT/Xpjrf3wy8rqqWJtmLISRP+N0a7POakenrgc2nV8U58XiG/4j2qKo/JVkBTNwxGD0GAf6zqt45WxWrqqsz3OH5W4Ze708CL2e4WP1y+3u1MTcMDfpyhuFgb2UIemvqEQzDcibGIW7FEAivBU6rNrwqyUUMQRKGC6yHtOntgE8m2YbhIuzHU5SxN7DryN/YWyfZoqqmfJh2shk+FtcCXxxp1zXtXDibG24hbwK8JclihnP9blPsp3ccpzoeo1Z1S37pSL22qKrfAr9Nck2S27Rl36uqi2HoRWe4a3A8cFCSIxh6+rZhuFt0Vtvmk5PKeSdwXFVN3CHsteVBwMfb39CfJZnO/2cbarthHre9qp6S5J4Mv/PPZbhwPXxV28w0h2GM38nA4yYCUAuRk20JXJ5hKMLjJ2Ym2bmqTq2qoxl64rZPchfg4qp6E8O4yN1nvAXr5kDgw1W1Y1UtqqrtGf7A/xp4bLtVc0fahQFDL+TCDOMIJx7iuMdcVHxtJbl7kl1GZi0GLpm02kYMxwSGuwP/1bbdN7nRLfrrGa6yAfZPcot27uwFnDb2yo/fVgy9hbDqC4avMvSmT9xq3mqmKzaLtgJ+0cLRQ4AdO+udBPxThjHOJNk2yapuR45FVV1fVafUMD76GQxDXc6pGx5QvGdVPaLVaSPgr4DfM/T2rqkw3Cqf2OdOVTURikcviP488v7P3PDczJsZepDvCRzJDRcbozZiGPIzUca2axqUJ8zgsfhTtS6x0TbWMLZ0oo3PBq5gCN5LmHq89KqO47oaPd6TfxYTdbvJhXqSnRiCysNqeJD589z45zL5YvjbwENyw9CycbTlQmCHJLdey+3g5t3uixjaveWk+Xsw9Nauzs257UNlqs6uqtczBOU5H55nWB6zqjqHYezt15OcyfAA22T/zvDQz7eAH47Mf3WGcXw/YDgJz2R4GOgHSZYz9IB8aAarPx2HctNe5E8z3I6+jOFBto8A32d4ePFahjD5ynacljM85HVzsAXwwQxP6Z7FcOV9zKR1fgfs2X6WDwVe2uY/gWHM8nLgwwwPxF3flp0FfA34LvCyqvrZjLZiPI4BPpXkdIaHNHuexfBH9WzgdIZjNl98FFjS2nYYN/6d/ov2n8bHgO+0dY9nuHCeMZ0Lu/PoX6g+uy3/R+D97YJ+TZwEPH1i/SR3S3Krtajqmlx0fYlhmButjMVrsf/ZPBY9WwGXtwD9BIZe7MmmexzX1Z5JdmoXCAczXNzfmuHv2G9aR8c+q9nHe4ETgeMyPNfQa8s3gIPbRfM23HB34Saq6vdtv29Msmnbz8LW4z8O62u7f8fwyUmvS3uYLsMHB/yRITeMw3rZ9iRbZLhLOWExN+2MmnUOw5iGqtqiM/+DDCf66LxjRqbfDrx9iu0eM8Xujm2v9VpV3eTkb73hpN0qbT2m32O4NURVLWe4PXOzUlWnM3Ww32tknd65ccgqdn1WVR02vdrNnKnaVFWfZbjjMXn+MZPeX8HwAOhku42s85rp13LmTT4ONXwaSm840W6T1n0j8MYZqtpUtgDe3G69XsfQU3cEw9Pmb2o9/AuANyS5jmG88J5V9dsk32B4WPnFa1DOexiGG3y/3TlZCRywFvU8huGi60qGu3M7TbHOvwBvbReoCxj+A16bj+ucrWPR8zbg0y30fJGphymt63HcvF2AT/hiVa3xR4kx3MV6C8On8XyN4ZNa/pzkDIaLv0tZg5BWVa9rx/HDDHdNF3HTtpzA0IFwLvAThnHhq/IihuEy5yb5I8NxO7otm8/tfgHwaobOlc3bfu4/cgdjvrY9wPOSvJPh+Y/fMcdDMMCvu9YsyPCRardhuO34qqr6wFzWZ32U4aPWrr65BEZJ0uxIcifgC8Dbq+pdc12fDZFhWZIkSepwGIYkqSvDp/FsNmn2E6rq7Lmoz1zyWMycDJ9ZPnn4zfOr6qS5qM9s2VDbDTevttuzLEmSJHX4aRiSJElSh2FZkiRJ6jAsS9IcSnJ9kuUjr0XrsI8Dksynz66WpPWGD/hJ0txa1dfWrqkDgM8xfI7pGkmyoKqum2a5kjTv2bMsSeuZJHsk+XqS05Oc1L71iiRPTXJakjOTfDrJLZM8ANiP4RtAlyfZOckpSZa0bbZOsqJNH55kaZKTga8muVWS9yX5XpIzkuzf1rtHm7c8yVm58TffSdIGxbAsSXNr85EhGCe0r4p9M3BgVe0BvA94RVv3M1X111V1L4avYn5yVX0bWAr8W1UtrqqLVlPefdq+Hwy8EDi5qvZk+AraV7evp30a8MbW472E4SvrJWmD5DAMSZpbNxqGkWQ3hq/J/vLwjbFsDFzeFu+W5OUM34i5BbAun0f65ar6dZt+BLBfkue297cAdmD4OtoXJtmOIaBfsA7lSNK8YFiWpPVLgHOq6v5TLPsAcEBVnZnkcGCvzj6u44Y7h7eYtOx3k8p6bFWdP2md89oXcOwLnJjkyKo6ec2bIEnzh8MwJGn9cj6wMMn9AZJskuQebdmWwOVtqMbjR7b5bVs2YQWwR5s+cBVlnQQ8M60LO8m92793AS6uqjcBnwV2n1aLJOlmzLAsSeuRqrqWIeC+MsmZwHLgAW3xvwOnAt8Cfjiy2SeAf2sP6e0MvAZ4epIzgK1XUdzLgE2As5Kc094DHAT8IMlyhiEhHxpD0yTpZsmvu5YkSZI67FmWJEmSOgzLkiRJUodhWZIkSeowLEuSJEkdhmVJkiSpw7AsSZIkdRiWJUmSpA7DsiRJktTx/wEsBTJn4muXcgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_importances(feature_names, importances, title=\"Average Feature Importances\", plot=True, axis_title=\"Features\"):\n",
    "    print(title)\n",
    "    for i in range(len(feature_names)):\n",
    "        print(feature_names[i], \": \", '%.3f'%(importances[i]))\n",
    "    x_pos = (np.arange(len(feature_names)))\n",
    "    if plot:\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.bar(x_pos, importances, align='center')\n",
    "        plt.xticks(x_pos, feature_names, wrap=True)\n",
    "        plt.xlabel(axis_title)\n",
    "        plt.title(title)\n",
    "visualize_importances(feature_names, np.mean(attr, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/codecarbon/viz/carbonboard.py:3: UserWarning: \n",
      "The dash_core_components package is deprecated. Please replace\n",
      "`import dash_core_components as dcc` with `from dash import dcc`\n",
      "  import dash_core_components as dcc\n",
      "/opt/conda/lib/python3.6/site-packages/codecarbon/viz/carbonboard.py:4: UserWarning: \n",
      "The dash_table package is deprecated. Please replace\n",
      "`import dash_table` with `from dash import dash_table`\n",
      "\n",
      "Also, if you're using any of the table format helpers (e.g. Group), replace \n",
      "`from dash_table.Format import Group` with \n",
      "`from dash.dash_table.Format import Group`\n",
      "  import dash_table as dt\n",
      "/opt/conda/lib/python3.6/site-packages/codecarbon/viz/components.py:5: UserWarning: \n",
      "The dash_html_components package is deprecated. Please replace\n",
      "`import dash_html_components as html` with `from dash import html`\n",
      "  import dash_html_components as html\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/carbonboard\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/codecarbon/viz/carbonboard.py\", line 279, in main\n",
      "    fire.Fire(viz)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/fire/core.py\", line 141, in Fire\n",
      "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/fire/core.py\", line 471, in _Fire\n",
      "    target=component.__name__)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/fire/core.py\", line 681, in _CallAndUpdateTrace\n",
      "    component = fn(*varargs, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/codecarbon/viz/carbonboard.py\", line 274, in viz\n",
      "    app = render_app(df)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/codecarbon/viz/carbonboard.py\", line 17, in render_app\n",
      "    header = components.get_header()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/codecarbon/viz/components.py\", line 33, in get_header\n",
      "    return dbc.Jumbotron(\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/dash_bootstrap_components/__init__.py\", line 52, in __getattr__\n",
      "    f\"{name} was deprecated in dash-bootstrap-components version \"\n",
      "AttributeError: Jumbotron was deprecated in dash-bootstrap-components version 1.0.0. You are using 1.0.3. For more details please see the migration guide: https://dash-bootstrap-components.opensource.faculty.ai/migration-guide/\n"
     ]
    }
   ],
   "source": [
    "!carbonboard --filepath=\"../output/emissions.csv\" --port=3333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.8-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
